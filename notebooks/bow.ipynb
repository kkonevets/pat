{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:48.018992Z",
     "start_time": "2017-07-24T13:35:30.521412Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import VocabTransform\n",
    "\n",
    "from common import *\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import copy \n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:01.309710Z",
     "start_time": "2017-07-24T13:34:57.636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_docs = glob.iglob(os.path.join(DATA_FOLDER, 'FIPS') + '/**/*.txt', recursive=True)\n",
    "# corpus_file = open(DATA_FOLDER + 'corpus.txt', 'w')\n",
    "# for item in all_docs:\n",
    "#     corpus_file.write(\"%s\\n\" % os.path.relpath(item, DATA_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:01.310573Z",
     "start_time": "2017-07-24T13:34:57.639Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize.toktok import ToktokTokenizer \n",
    "# toktok = ToktokTokenizer()\n",
    "# toktok.tokenize('устройство по п . 3 , отличать тем , что оно снабжать устанавливать в головка винт , в который размещать шарик и ролик .  . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:01.311422Z",
     "start_time": "2017-07-24T13:34:57.643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:01.312125Z",
     "start_time": "2017-07-24T13:34:57.646Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_docs = get_all_docs(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:01.312861Z",
     "start_time": "2017-07-24T13:34:57.657Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallelizer = Parallel(n_jobs=cpu_count)\n",
    "\n",
    "# this iterator returns the functions to execute for each task\n",
    "tasks_iterator = ( delayed(save_corpus)(list_block, os.path.join(DATA_FOLDER, 'BoW'), i) for \n",
    "                  i, list_block in enumerate(grouper(len(all_docs)//100, all_docs)) ) \n",
    "result = parallelizer( tasks_iterator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:35:49.887770Z",
     "start_time": "2017-07-24T13:35:49.868073Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corps = glob.glob(join(DATA_FOLDER, 'BoW/*corpus.mm'))\n",
    "dicts = glob.glob(join(DATA_FOLDER, 'BoW/*.dict'))\n",
    "corps.sort(key=natural_keys)\n",
    "dicts.sort(key=natural_keys)\n",
    "pairs = list(zip(dicts, corps))\n",
    "result = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T13:53:05.260750Z",
     "start_time": "2017-07-24T13:35:51.660084Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/BoW/0.dict\n",
      "../data/BoW/1.dict\n",
      "../data/BoW/2.dict\n",
      "../data/BoW/3.dict\n",
      "../data/BoW/4.dict\n",
      "../data/BoW/5.dict\n",
      "../data/BoW/6.dict\n",
      "../data/BoW/7.dict\n",
      "../data/BoW/8.dict\n",
      "../data/BoW/9.dict\n",
      "../data/BoW/10.dict\n",
      "../data/BoW/11.dict\n",
      "../data/BoW/12.dict\n",
      "../data/BoW/13.dict\n",
      "../data/BoW/14.dict\n",
      "../data/BoW/15.dict\n",
      "../data/BoW/16.dict\n",
      "../data/BoW/17.dict\n",
      "../data/BoW/18.dict\n",
      "../data/BoW/19.dict\n",
      "../data/BoW/20.dict\n",
      "../data/BoW/21.dict\n",
      "../data/BoW/22.dict\n",
      "../data/BoW/23.dict\n",
      "../data/BoW/24.dict\n",
      "../data/BoW/25.dict\n",
      "../data/BoW/26.dict\n",
      "../data/BoW/27.dict\n",
      "../data/BoW/28.dict\n",
      "../data/BoW/29.dict\n",
      "../data/BoW/30.dict\n",
      "../data/BoW/31.dict\n",
      "../data/BoW/32.dict\n",
      "../data/BoW/33.dict\n",
      "../data/BoW/34.dict\n",
      "../data/BoW/35.dict\n",
      "../data/BoW/36.dict\n",
      "../data/BoW/37.dict\n",
      "../data/BoW/38.dict\n",
      "../data/BoW/39.dict\n",
      "../data/BoW/40.dict\n",
      "../data/BoW/41.dict\n",
      "../data/BoW/42.dict\n",
      "../data/BoW/43.dict\n",
      "../data/BoW/44.dict\n",
      "../data/BoW/45.dict\n",
      "../data/BoW/46.dict\n",
      "../data/BoW/47.dict\n",
      "../data/BoW/48.dict\n",
      "../data/BoW/49.dict\n",
      "../data/BoW/50.dict\n",
      "../data/BoW/51.dict\n",
      "../data/BoW/52.dict\n",
      "../data/BoW/53.dict\n",
      "../data/BoW/54.dict\n",
      "../data/BoW/55.dict\n",
      "../data/BoW/56.dict\n",
      "../data/BoW/57.dict\n",
      "../data/BoW/58.dict\n",
      "../data/BoW/59.dict\n",
      "../data/BoW/60.dict\n",
      "../data/BoW/61.dict\n",
      "../data/BoW/62.dict\n",
      "../data/BoW/63.dict\n",
      "../data/BoW/64.dict\n",
      "../data/BoW/65.dict\n",
      "../data/BoW/66.dict\n",
      "../data/BoW/67.dict\n",
      "../data/BoW/68.dict\n",
      "../data/BoW/69.dict\n",
      "../data/BoW/70.dict\n",
      "../data/BoW/71.dict\n",
      "../data/BoW/72.dict\n",
      "../data/BoW/73.dict\n",
      "../data/BoW/74.dict\n",
      "../data/BoW/75.dict\n",
      "../data/BoW/76.dict\n",
      "../data/BoW/77.dict\n",
      "../data/BoW/78.dict\n",
      "../data/BoW/79.dict\n",
      "../data/BoW/80.dict\n",
      "../data/BoW/81.dict\n",
      "../data/BoW/82.dict\n",
      "../data/BoW/83.dict\n",
      "../data/BoW/84.dict\n",
      "../data/BoW/85.dict\n",
      "../data/BoW/86.dict\n",
      "../data/BoW/87.dict\n",
      "../data/BoW/88.dict\n",
      "../data/BoW/89.dict\n",
      "../data/BoW/90.dict\n",
      "../data/BoW/91.dict\n",
      "../data/BoW/92.dict\n",
      "../data/BoW/93.dict\n",
      "../data/BoW/94.dict\n",
      "../data/BoW/95.dict\n",
      "../data/BoW/96.dict\n",
      "../data/BoW/97.dict\n",
      "../data/BoW/98.dict\n",
      "../data/BoW/99.dict\n",
      "../data/BoW/100.dict\n",
      "CPU times: user 17min 2s, sys: 10.5 s, total: 17min 12s\n",
      "Wall time: 17min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merged_corpus = None\n",
    "for dic_name, corp_name in result:\n",
    "    cur_dict = corpora.Dictionary.load(dic_name)\n",
    "    cur_corp = corpora.MmCorpus(corp_name)\n",
    "    \n",
    "    print(dic_name)\n",
    "    if not merged_corpus:\n",
    "        dict1 = cur_dict\n",
    "        merged_corpus = cur_corp\n",
    "        continue\n",
    "\n",
    "    cur_to_dict1 = dict1.merge_with(cur_dict)\n",
    "    merged_corpus = itertools.chain(merged_corpus, cur_to_dict1[cur_corp])\n",
    "    \n",
    "dict1.save(join(DATA_FOLDER, 'old.dict'))\n",
    "corpora.MmCorpus.serialize(join(DATA_FOLDER, 'corpus.mm'), merged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in less than 5 documents (absolute number) or more than 50% documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# filter the dictionary\n",
    "old_dict = corpora.Dictionary.load(join(DATA_FOLDER, 'old.dict'))\n",
    "new_dict = copy.deepcopy(old_dict)\n",
    "new_dict.filter_extremes(no_below=3, keep_n=None)\n",
    "new_dict.save(join(DATA_FOLDER, 'filtered.dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# now transform the corpus\n",
    "corpus = corpora.MmCorpus(join(DATA_FOLDER, 'corpus.mm'))\n",
    "old2new = {old_dict.token2id[token]:new_id for new_id, token in new_dict.iteritems()}\n",
    "vt = VocabTransform(old2new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "corpora.MmCorpus.serialize(join(DATA_FOLDER, 'filtered_corpus.mm'), vt[corpus], id2word=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '../data/*corpus*': No such file or directory\n",
      "cp: cannot stat '../data/*.dict': No such file or directory\n",
      "cp: cannot stat '../data/all_docs.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp ../data/*corpus* ~/Yandex.Disk\n",
    "!cp ../data/*.dict ~/Yandex.Disk\n",
    "!cp ../data/all_docs.txt ~/Yandex.Disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
