{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import VocabTransform\n",
    "\n",
    "from common import *\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import copy \n",
    "\n",
    "cpu_count = multiprocessing.cpu_count() -1\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_docs = glob.iglob(os.path.join(DATA_FOLDER, 'FIPS') + '/**/*.txt', recursive=True)\n",
    "# corpus_file = open(DATA_FOLDER + 'corpus.txt', 'w')\n",
    "# for item in all_docs:\n",
    "#     corpus_file.write(\"%s\\n\" % os.path.relpath(item, DATA_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize.toktok import ToktokTokenizer \n",
    "# toktok = ToktokTokenizer()\n",
    "# toktok.tokenize('устройство по п . 3 , отличать тем , что оно снабжать устанавливать в головка винт , в который размещать шарик и ролик .  . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 10.3 s, total: 41.6 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fname = join(DATA_FOLDER, 'all_docs.txt')\n",
    "\n",
    "if not exists(fname):\n",
    "    all_docs = glob.glob(join(DATA_FOLDER, 'FIPS') + '/**/*.txt', recursive=True)\n",
    "    with open(fname, mode='wt', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(all_docs))\n",
    "else:\n",
    "    with open(fname, mode='r', encoding='utf-8') as f:\n",
    "        all_docs = f.read().splitlines()\n",
    "\n",
    "# dfolder = os.path.join(DATA_FOLDER, 'FIPS/Inventions patents_txt_output/0c/0c')\n",
    "# all_docs = glob.glob(dfolder + '/**/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11946/11946 [18:06<00:00, 11.00it/s] \n",
      "100%|██████████| 11946/11946 [18:17<00:00,  6.54it/s]\n",
      "100%|██████████| 11946/11946 [18:34<00:00, 13.56it/s]\n",
      "100%|██████████| 11946/11946 [18:37<00:00, 10.69it/s]\n",
      "100%|██████████| 11946/11946 [18:39<00:00, 10.67it/s]\n",
      "100%|██████████| 11946/11946 [18:42<00:00,  7.65it/s]\n",
      "  2%|▏         | 208/11946 [00:14<18:25, 10.62it/s]s]\n",
      "100%|██████████| 11946/11946 [19:00<00:00, 10.47it/s]\n",
      "100%|██████████| 11946/11946 [19:07<00:00, 10.41it/s]\n",
      " 96%|█████████▌| 11459/11946 [19:15<01:19,  6.12it/s]\n",
      " 99%|█████████▉| 11879/11946 [19:16<00:07,  9.16it/s]\n",
      "100%|██████████| 11946/11946 [19:19<00:00, 10.31it/s]\n",
      "100%|██████████| 11946/11946 [19:21<00:00, 10.29it/s]\n",
      " 99%|█████████▊| 11778/11946 [19:23<00:16, 10.49it/s]\n",
      "100%|██████████| 11946/11946 [19:25<00:00, 14.14it/s]\n",
      "100%|██████████| 11946/11946 [19:28<00:00, 10.22it/s]\n",
      "100%|██████████| 11946/11946 [19:30<00:00, 10.20it/s]\n",
      "  1%|          | 142/11946 [00:12<24:14,  8.12it/s]s]\n",
      "100%|██████████| 11946/11946 [19:37<00:00, 10.14it/s]\n",
      " 97%|█████████▋| 11543/11946 [19:41<00:47,  8.46it/s]\n",
      "100%|██████████| 11946/11946 [19:45<00:00, 10.08it/s]\n",
      "100%|██████████| 11946/11946 [19:44<00:00, 10.08it/s]\n",
      "  8%|▊         | 905/11946 [01:11<17:19, 10.62it/s]s] \n",
      "100%|██████████| 11946/11946 [19:56<00:00, 12.95it/s] \n",
      "  1%|          | 71/11946 [00:07<17:58, 11.02it/s]/s]\n",
      "100%|██████████| 11946/11946 [20:09<00:00, 22.32it/s]\n",
      "100%|██████████| 11946/11946 [20:10<00:00,  9.87it/s]\n",
      "  5%|▍         | 539/11946 [00:45<18:21, 10.36it/s]s]\n",
      "100%|██████████| 11946/11946 [20:24<00:00,  6.51it/s]\n",
      " 69%|██████▉   | 8270/11946 [11:23<09:43,  6.30it/s]  "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parallelizer = Parallel(n_jobs=cpu_count)\n",
    "\n",
    "# this iterator returns the functions to execute for each task\n",
    "tasks_iterator = ( delayed(save_corpus)(list_block, os.path.join(DATA_FOLDER, 'BoW'), i) for \n",
    "                  i, list_block in enumerate(grouper(len(all_docs)//100, all_docs)) ) \n",
    "result = parallelizer( tasks_iterator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corps = glob.glob(join(DATA_FOLDER, 'BoW/*corpus.mm'))\n",
    "dicts = glob.glob(join(DATA_FOLDER, 'BoW/*.dict'))\n",
    "corps.sort(key=natural_keys)\n",
    "dicts.sort(key=natural_keys)\n",
    "pairs = list(zip(dicts, corps))\n",
    "result = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "merged_corpus = None\n",
    "for dic_name, corp_name in result:\n",
    "    cur_dict = corpora.Dictionary.load(dic_name)\n",
    "    cur_corp = corpora.MmCorpus(corp_name)\n",
    "    \n",
    "    print(dic_name)\n",
    "    if not merged_corpus:\n",
    "        dict1 = cur_dict\n",
    "        merged_corpus = cur_corp\n",
    "        continue\n",
    "\n",
    "    cur_to_dict1 = dict1.merge_with(cur_dict)\n",
    "    merged_corpus = itertools.chain(merged_corpus, cur_to_dict1[cur_corp])\n",
    "    \n",
    "dict1.save(join(DATA_FOLDER, 'old.dict'))\n",
    "corpora.MmCorpus.serialize(join(DATA_FOLDER, 'corpus.mm'), merged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in less than 5 documents (absolute number) or more than 50% documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# filter the dictionary\n",
    "old_dict = corpora.Dictionary.load(join(DATA_FOLDER, 'old.dict'))\n",
    "new_dict = copy.deepcopy(old_dict)\n",
    "new_dict.filter_extremes(no_below=3, keep_n=None)\n",
    "new_dict.save(join(DATA_FOLDER, 'filtered.dict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# now transform the corpus\n",
    "corpus = corpora.MmCorpus(join(DATA_FOLDER, 'corpus.mm'))\n",
    "old2new = {old_dict.token2id[token]:new_id for new_id, token in new_dict.iteritems()}\n",
    "vt = VocabTransform(old2new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpora.MmCorpus.serialize(join(DATA_FOLDER, 'filtered_corpus.mm'), vt[corpus], id2word=new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp ../data/*corpus* ~/Yandex.Disk\n",
    "!cp ../data/*.dict ~/Yandex.Disk\n",
    "!cp ../data/all_docs.txt ~/Yandex.Disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
