{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предполагаем что все данные лежат в папке data.\n",
    "Исходные данные представляют из себя лемматизированные тексты патентов в папке data/raw. Сколько патентов, столько и файлов. Ниже приводится процедура по конвертированию этих файлов в один большой. Работа с одним файлом удобна для разных алгоритмов и влияет на скорость чтения данных с диска. Слова в текстах отделены пробелами, а предложения точками. Причем эти тексты результат обработки лемматизатором, который кроме прочего отделяет знаки препинания от слов и отделяет их пробелами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# получаем имена файлов в дирректории\n",
    "fnames = glob('./raw/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process - функция, которая обрабатывает часть файлов и сохраняет результат обработки ввиде одного файла с именем data/corpus_json.txt. Функция вначале разбивает текст на предложения, затем для каждого слова в предложении проверяет что оно не является стоп словом и что длина слова больше 1. Обработанный таким образом документ представляет список предложений, каждое из которых является саиском слов. Затем обработанный таким образом документ сохраняется ввиде стори json: (document_id, document) и сохраняется в файл data/i.txt ввиде строки json и так для каждого документа.\n",
    "\n",
    "Стоп слова хранятся в файле data/StopWords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "from os.path import basename\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "with io.open(join(cur_dir, 'StopWords.txt'), 'r', encoding='utf8') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "\n",
    "def process(fnames):\n",
    "    with io.open('../data/corpus.txt', 'w', encoding='utf8') as fw:\n",
    "        for fn in fnames:\n",
    "            with io.open(fn, encoding='utf8') as fr:\n",
    "                text = fr.read()\n",
    "            sents = sent_tokenize(text)\n",
    "            sents = [[w for w in s.split() if w not in stop_list and len(w)>1] \n",
    "                     for s in sents]\n",
    "            sents = [s for s in sents if len(s)]\n",
    "            s = json.dumps((basename(fn).split('.')[0], sents))            \n",
    "            fw.write(s + u'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем файл corpus_json.txt сжимается с помощью gzip для уменьшения занимаемого места на диске, сжатие файла не влияет на скорость его дальнейшего чтения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приводятся процедуры для чтения файла corpus_json.txt.gz с помощью gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "import gensim\n",
    "from gzip import GzipFile\n",
    "\n",
    "def iter_docs():\n",
    "    with GzipFile('../data/corpus_json.txt.gz', 'r') as fr:\n",
    "        for line in fr:\n",
    "            _id, sents = ujson.loads(line)\n",
    "            yield _id, sents\n",
    "    \n",
    "def iter_sents():\n",
    "    for _id, sents in iter_docs():\n",
    "        for s in sents:\n",
    "            yield s\n",
    "                \n",
    "class Sentences(object):\n",
    "    def __iter__(self):\n",
    "        for sent in iter_sents():\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(iter_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем биграммы на диск для дальнейшего использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram.save('../data/bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть полученные биграммы для первых 100 предложений можно так: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for phrase, score in bigram.export_phrases(islice(iter_sents(), 100)):\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы преобразуем биграммы в формат наиболее быстрый для их применения к тексту и сохраняем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_ph = gensim.models.phrases.Phraser(bigram)\n",
    "bigram_ph.save('../data/bigram_ph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем триграммы как результат применения процедуры получения биграмм к ранее полученному тексту с биграммами. Когда биграмма объединяется с одним словом получается триграмма. А когда объединяются две биграммы, получается 4-х грамма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(bigram_ph[iter_sents()])\n",
    "trigram.save('../data/trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таже процедура применяется к триграммам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_ph = gensim.models.phrases.Phraser(trigram)\n",
    "trigram_ph.save('../data/trigram_ph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор триграмм "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент оперативная память компьютера может быть уже заполнена, поэтому есть смысл завершить процесс и открыть новый, загрузив в него trigram_ph, который не занимает много места:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_ph = gensim.models.phrases.Phrases.load('../data/trigram_ph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В trigram_ph.phrasegrams.items() хранятся пары ((слово1, слово2), (tf, score)), где tf - term friquency, т.е. частота встречи слова во всем корпусе, score - это метрика для оценки того насколько часто слова входящие в n-грамму встречаются совместно нежели по отдельности. Эта метрика считается по формуле\n",
    "\n",
    "(count(worda followed by wordb) - min_count) * N / (count(worda) * count(wordb)) > threshold, where N is the total vocabulary size\n",
    "\n",
    "В цикле каждая n-грамма отбирается по погрогам min_count, threshold. Значения порогов выбираются на усмотрение по качеству получаемых результатов. Также проверяется что в каждом составляющем слове есть хотябы одна буква."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "min_count, min_score = 5, 10\n",
    "\n",
    "_list = []\n",
    "for k,v in trigram_ph.phrasegrams.items():\n",
    "    if v[0] > min_count and v[1] > threshold:\n",
    "        the_string = '_'.join(k)\n",
    "        if re.search('[а-яА-Яa-zA-Z]', k[0]) and re.search('[а-яА-Яa-zA-Z]', k[1]):\n",
    "            _list.append([the_string, v[0], v[1]])\n",
    "        \n",
    "df = pd.DataFrame(_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверить количество полученных триграмм можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df[0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем коллокации в архив, отсортированные по имени для удобства просмотра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with GzipFile('../data/collocations_trigrams.txt.gz', 'w') as f:\n",
    "    for tag in sorted(df[0].unique()):\n",
    "        f.write(tag + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение классов коллокаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда min_count и threshold подобраны, можно начать получение уже итогового trigram_ph с фиксированными min_count и threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
