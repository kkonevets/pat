{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:09.939616Z",
     "start_time": "2017-08-02T21:06:51.047403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:10.197278Z",
     "start_time": "2017-08-02T21:07:09.940935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_docs = get_all_docs(DATA_FOLDER)\n",
    "val_docs = sorted(glob(join(DATA_FOLDER, 'validate/*.txt')))\n",
    "ziped_files = sorted(glob(DATA_FOLDER + '/documents/*.gz'), key=natural_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:14.382564Z",
     "start_time": "2017-08-02T21:07:10.198713Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 00:07:10,200 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-03 00:07:12,369 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-03 00:07:12,370 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-03 00:07:12,532 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-03 00:07:12,533 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-03 00:07:12,685 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-03 00:07:12,686 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))\n",
    "wv = w2v_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save corpus as single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:14.397047Z",
     "start_time": "2017-08-02T21:07:14.383962Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_corpus(fname, ziped_files, wv):\n",
    "    with open(fname, 'w') as f:\n",
    "        for fn in tqdm(ziped_files):\n",
    "            with GzipFile(fn, 'r') as gzf:\n",
    "                text = gzf.read()\n",
    "            docs = json.loads(text)\n",
    "            for doc in docs:\n",
    "                sents = [(' ').join([str(wv.vocab[w].index) for w in sent if w in wv]) for sent in doc]\n",
    "                doc_str = (' . ').join([s for s in sents if s != ''])\n",
    "                f.write(doc_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:14.405210Z",
     "start_time": "2017-08-02T21:07:14.398468Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus_file = join(DATA_FOLDER, \"corpus.txt\")\n",
    "# save_corpus(corpus_file, ziped_files, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over corpus efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:32.109938Z",
     "start_time": "2017-08-02T21:07:32.094085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_docs(corpus_file, wv, batch_size=100, chunk_size=10):\n",
    "    \"\"\"\n",
    "    batch_size - number of file lines in one chunk\n",
    "    \"\"\"\n",
    "    with GzipFile(corpus_file, 'r') as f:\n",
    "        while True:\n",
    "            chunk_lines = list(islice(f, batch_size*chunk_size))\n",
    "            if not chunk_lines:\n",
    "                return\n",
    "            \n",
    "            for batch_lines in np.array_split(chunk_lines, batch_size):\n",
    "                for line in batch_lines:\n",
    "                    doc = line.decode('utf-8').split('.')\n",
    "                    vectorized_doc = [wv.syn0[[int(ix) for ix in sent.split()]] for sent in doc]\n",
    "                    yield vectorized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:07:14.438439Z",
     "start_time": "2017-08-02T21:07:14.421081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batches(corpus_file, wv, batch_size):\n",
    "    it = iterate_docs(corpus_file, wv, batch_size)\n",
    "    while True:\n",
    "        batch = tuple(islice(it, batch_size))\n",
    "        if not batch:\n",
    "            return\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:20:34.816789Z",
     "start_time": "2017-08-02T21:07:33.087539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n",
      "21000\n",
      "24000\n",
      "27000\n",
      "30000\n",
      "33000\n",
      "36000\n",
      "39000\n",
      "42000\n",
      "45000\n",
      "48000\n",
      "51000\n",
      "54000\n",
      "57000\n",
      "60000\n",
      "63000\n",
      "66000\n",
      "69000\n",
      "72000\n",
      "75000\n",
      "78000\n",
      "81000\n",
      "84000\n",
      "87000\n",
      "90000\n",
      "93000\n",
      "96000\n",
      "99000\n",
      "102000\n",
      "105000\n",
      "108000\n",
      "111000\n",
      "114000\n",
      "117000\n",
      "120000\n",
      "123000\n",
      "126000\n",
      "129000\n",
      "132000\n",
      "135000\n",
      "138000\n",
      "141000\n",
      "144000\n",
      "147000\n",
      "150000\n",
      "153000\n",
      "156000\n",
      "159000\n",
      "162000\n",
      "165000\n",
      "168000\n",
      "171000\n",
      "174000\n",
      "177000\n",
      "180000\n",
      "183000\n",
      "186000\n",
      "189000\n",
      "192000\n",
      "195000\n",
      "198000\n",
      "201000\n",
      "204000\n",
      "207000\n",
      "210000\n",
      "213000\n",
      "216000\n",
      "219000\n",
      "222000\n",
      "225000\n",
      "228000\n",
      "231000\n",
      "234000\n",
      "237000\n",
      "240000\n",
      "243000\n",
      "246000\n",
      "249000\n",
      "252000\n",
      "255000\n",
      "258000\n",
      "261000\n",
      "264000\n",
      "267000\n",
      "270000\n",
      "273000\n",
      "276000\n",
      "279000\n",
      "282000\n",
      "285000\n",
      "288000\n",
      "291000\n",
      "294000\n",
      "297000\n",
      "300000\n",
      "303000\n",
      "306000\n",
      "309000\n",
      "312000\n",
      "315000\n",
      "318000\n",
      "321000\n",
      "324000\n",
      "327000\n",
      "330000\n",
      "333000\n",
      "336000\n",
      "339000\n",
      "342000\n",
      "345000\n",
      "348000\n",
      "351000\n",
      "354000\n",
      "357000\n",
      "360000\n",
      "363000\n",
      "366000\n",
      "369000\n",
      "372000\n",
      "375000\n",
      "378000\n",
      "381000\n",
      "384000\n",
      "387000\n",
      "390000\n",
      "393000\n",
      "396000\n",
      "399000\n",
      "402000\n",
      "405000\n",
      "408000\n",
      "411000\n",
      "414000\n",
      "417000\n",
      "420000\n",
      "423000\n",
      "426000\n",
      "429000\n",
      "432000\n",
      "435000\n",
      "438000\n",
      "441000\n",
      "444000\n",
      "447000\n",
      "450000\n",
      "453000\n",
      "456000\n",
      "459000\n",
      "462000\n",
      "465000\n",
      "468000\n",
      "471000\n",
      "474000\n",
      "477000\n",
      "480000\n",
      "483000\n",
      "486000\n",
      "489000\n",
      "492000\n",
      "495000\n",
      "498000\n",
      "501000\n",
      "504000\n",
      "507000\n",
      "510000\n",
      "513000\n",
      "516000\n",
      "519000\n",
      "522000\n",
      "525000\n",
      "528000\n",
      "531000\n",
      "534000\n",
      "537000\n",
      "540000\n",
      "543000\n",
      "546000\n",
      "549000\n",
      "552000\n",
      "555000\n",
      "558000\n",
      "561000\n",
      "564000\n",
      "567000\n",
      "570000\n",
      "573000\n",
      "576000\n",
      "579000\n",
      "582000\n",
      "585000\n",
      "588000\n",
      "591000\n",
      "594000\n",
      "597000\n",
      "600000\n",
      "603000\n",
      "606000\n",
      "609000\n",
      "612000\n",
      "615000\n",
      "618000\n",
      "621000\n",
      "624000\n",
      "627000\n",
      "630000\n",
      "633000\n",
      "636000\n",
      "639000\n",
      "642000\n",
      "645000\n",
      "648000\n",
      "651000\n",
      "654000\n",
      "657000\n",
      "660000\n",
      "663000\n",
      "666000\n",
      "669000\n",
      "672000\n",
      "675000\n",
      "678000\n",
      "681000\n",
      "684000\n",
      "687000\n",
      "690000\n",
      "693000\n",
      "696000\n",
      "699000\n",
      "702000\n",
      "705000\n",
      "708000\n",
      "711000\n",
      "714000\n",
      "717000\n",
      "720000\n",
      "723000\n",
      "726000\n",
      "729000\n",
      "732000\n",
      "735000\n",
      "738000\n",
      "741000\n",
      "744000\n",
      "747000\n",
      "750000\n",
      "753000\n",
      "756000\n",
      "759000\n",
      "762000\n",
      "765000\n",
      "768000\n",
      "771000\n",
      "774000\n",
      "777000\n",
      "780000\n",
      "783000\n",
      "786000\n",
      "789000\n",
      "792000\n",
      "795000\n",
      "798000\n",
      "801000\n",
      "804000\n",
      "807000\n",
      "810000\n",
      "813000\n",
      "816000\n",
      "819000\n",
      "822000\n",
      "825000\n",
      "828000\n",
      "831000\n",
      "834000\n",
      "837000\n",
      "840000\n",
      "843000\n",
      "846000\n",
      "849000\n",
      "852000\n",
      "855000\n",
      "858000\n",
      "861000\n",
      "864000\n",
      "867000\n",
      "870000\n",
      "873000\n",
      "876000\n",
      "879000\n",
      "882000\n",
      "885000\n",
      "888000\n",
      "891000\n",
      "894000\n",
      "897000\n",
      "900000\n",
      "903000\n",
      "906000\n",
      "909000\n",
      "912000\n",
      "915000\n",
      "918000\n",
      "921000\n",
      "924000\n",
      "927000\n",
      "930000\n",
      "933000\n",
      "936000\n",
      "939000\n",
      "942000\n",
      "945000\n",
      "948000\n",
      "951000\n",
      "954000\n",
      "957000\n",
      "960000\n",
      "963000\n",
      "966000\n",
      "969000\n",
      "972000\n",
      "975000\n",
      "978000\n",
      "981000\n",
      "984000\n",
      "987000\n",
      "990000\n",
      "993000\n",
      "996000\n",
      "999000\n",
      "1002000\n",
      "1005000\n",
      "1008000\n",
      "1011000\n",
      "1014000\n",
      "1017000\n",
      "1020000\n",
      "1023000\n",
      "1026000\n",
      "1029000\n",
      "1032000\n",
      "1035000\n",
      "1038000\n",
      "1041000\n",
      "1044000\n",
      "1047000\n",
      "1050000\n",
      "1053000\n",
      "1056000\n",
      "1059000\n",
      "1062000\n",
      "1065000\n",
      "1068000\n",
      "1071000\n",
      "1074000\n",
      "1077000\n",
      "1080000\n",
      "1083000\n",
      "1086000\n",
      "1089000\n",
      "1092000\n",
      "1095000\n",
      "1098000\n",
      "1101000\n",
      "1104000\n",
      "1107000\n",
      "1110000\n",
      "1113000\n",
      "1116000\n",
      "1119000\n",
      "1122000\n",
      "1125000\n",
      "1128000\n",
      "1131000\n",
      "1134000\n",
      "1137000\n",
      "1140000\n",
      "1143000\n",
      "1146000\n",
      "1149000\n",
      "1152000\n",
      "1155000\n",
      "1158000\n",
      "1161000\n",
      "1164000\n",
      "1167000\n",
      "1170000\n",
      "1173000\n",
      "1176000\n",
      "1179000\n",
      "1182000\n",
      "1185000\n",
      "1188000\n",
      "1191000\n",
      "1194000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "corpus_file = join(DATA_FOLDER, \"corpus.txt.gz\")\n",
    "\n",
    "gen = gen_batches(corpus_file, wv, batch_size)\n",
    "\n",
    "# islice(gen, len(all_docs) - 10, None)\n",
    "tot = 0\n",
    "for batch in gen:\n",
    "    tot += len(batch)\n",
    "    if tot % 3000 == 0:\n",
    "        print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cursor = db.patents.find({'similar': {'$exists': True}}, \n",
    "                       {'similar': 1})\n",
    "similar = []\n",
    "for doc in cursor:\n",
    "    similar.append((str(doc['_id']), doc['similar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368458"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5984b7c2b6b1132856638528', ['5984b65cb6b1131291638512']),\n",
       " ('5984b980b6b113401d638515', ['5984b65cb6b113129b638521']),\n",
       " ('5984b9f9b6b11348ca63850f', ['5984b71fb6b1131bef638550']),\n",
       " ('5984b9fab6b11348c163851e', ['5984b65ab6b113129b638508']),\n",
       " ('5984ba05b6b11348c1638540', ['5984b680b6b1131490638515']),\n",
       " ('5984ba11b6b11349ec6384f0',\n",
       "  ['5984b688b6b113147e638533',\n",
       "   '5984b5dbb6b1130839638502',\n",
       "   '5984b832b6b1132dc8638509']),\n",
       " ('5984ba15b6b11349ee638507', ['5984b78bb6b113254c63852e']),\n",
       " ('5984ba15b6b11349ee638508', ['5984b584b6b11303c1638551']),\n",
       " ('5984ba17b6b11349f563850c', ['5984b787b6b113256063850b']),\n",
       " ('5984ba17b6b11349f563850d', ['5984b5ccb6b113073663853e'])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, k-max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size, \n",
    "                 filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        self.W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                             trainable=False, name=\"W\")\n",
    "\n",
    "        self.embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_size])\n",
    "        self.embedding_init = self.W.assign(self.embedding_placeholder)\n",
    "        \n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.X = tf.placeholder(tf.int32, [None, None], name=\"X\")\n",
    "        self.y = tf.placeholder(tf.float32, [None, 2], name=\"y\")\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_x_5:0' shape=(?, 100, 300) dtype=int32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", wv.syn0[1], \"Dimensionality of word embedding\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# train/dev split here\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "#  If you would like TensorFlow to automatically choose an existing and supported device to \n",
    "#  run the operations in case the specified one doesn't exist, you can set allow_soft_placement to True\n",
    "    \n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Assign word embeddings to variable\n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: wv.syn0})\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
