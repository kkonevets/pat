{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "from tflearn.layers.core import time_distributed\n",
    "from tflearn.layers import conv_2d\n",
    "from io import StringIO\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "os.environ['PYTHONASYNCIODEBUG'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    strings = tf.string_split([text], delimiter='\\n')\n",
    "    raw_nums = tf.string_split(strings.values)\n",
    "    nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "    dense = tf.sparse_to_dense(raw_nums.indices, \n",
    "                               raw_nums.dense_shape, \n",
    "                               nums,\n",
    "                               default_value=0)\n",
    "#     dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    fnames = filename_queue.dequeue()\n",
    "    example = []\n",
    "    for fn in tf.unstack(fnames):\n",
    "        record_string = tf.read_file(fn)\n",
    "        arr = parse_csv(record_string)\n",
    "        example.append(arr)\n",
    "    return example\n",
    "\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        filenames, \n",
    "        num_epochs=num_epochs, \n",
    "        capacity=32,\n",
    "        shuffle=True,\n",
    "        seed=0)\n",
    "    example = read_input_tuple(filename_queue)    \n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    positive, negative, anchor = tf.train.batch(\n",
    "        example, \n",
    "        batch_size=batch_size, \n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "#         allow_smaller_final_batch=True,\n",
    "        num_threads=cpu_count\n",
    "    )\n",
    "    return positive, negative, anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = glob('../data/corpus/*.txt')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = np.reshape(filenames[:-(len(filenames)%3)], (-1,3))\n",
    "# filenames = np.reshape(filenames, (-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(64, 123, 40), (64, 123, 40), (64, 123, 40)]\n",
      "[(64, 123, 40), (64, 123, 40), (64, 123, 40)]\n",
      "[(64, 123, 40), (64, 123, 40), (64, 123, 40)]\n",
      "[(64, 123, 40), (64, 123, 40), (64, 123, 40)]\n",
      "[(64, 123, 40), (64, 123, 40), (64, 123, 40)]\n",
      "Done training -- epoch limit reached\n",
      "--- 0.16971611976623535 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():  \n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        res = input_pipeline(filenames, batch_size=64, num_epochs=1)        \n",
    "        \n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "                \n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "#                 batches, keys = sess.run([example_batch, label_batch])  \n",
    "                [fnq] = sess.run([res])  \n",
    "#                 time_distributed(batches, conv_2d, [num_filters, filter_sizes, strides])\n",
    "                print([t.shape for t in fnq])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)        \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
