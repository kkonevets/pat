{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "from asyncio.events import  AbstractEventLoop\n",
    "import logging\n",
    "import os, time\n",
    "from numba import jit\n",
    "import tflearn\n",
    "from tflearn.layers.core import time_distributed\n",
    "from tflearn.layers import conv_2d\n",
    "from io import StringIO\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "os.environ['PYTHONASYNCIODEBUG'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    strings = tf.string_split([text], delimiter='\\n')\n",
    "    raw_nums = tf.string_split(strings.values)\n",
    "    nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "\n",
    "    dense = tf.sparse_to_dense(raw_nums.indices, \n",
    "                               raw_nums.dense_shape, \n",
    "                               nums,\n",
    "                               default_value=0)\n",
    "#     dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "\n",
    "def read_input_file(filename_queue):\n",
    "    reader = tf.WholeFileReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example = parse_csv(record_string)\n",
    "#     processed_example = some_processing(example)\n",
    "    return example, key\n",
    "\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=False, seed=0)\n",
    "    example, key = read_input_file(filename_queue)    \n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.batch(\n",
    "        [example, key], \n",
    "        batch_size=batch_size, \n",
    "        capacity=capacity,\n",
    "#         min_after_dequeue=min_after_dequeue,\n",
    "        dynamic_pad=True,\n",
    "        allow_smaller_final_batch=True,\n",
    "        num_threads=cpu_count\n",
    "    )\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('../data/corpus/*.txt')[100:112]\n",
    "filenames = np.reshape(filenames, (-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['../data/corpus/5984bf58b6b11307a2638529.txt',\n",
       "        '../data/corpus/5984c732b6b113644f638532.txt',\n",
       "        '../data/corpus/5984bd1eb6b1136c756384f5.txt'],\n",
       "       ['../data/corpus/5984c1fdb6b1132b6863853e.txt',\n",
       "        '../data/corpus/5984cfd2b6b113399a6384f7.txt',\n",
       "        '../data/corpus/5984c4eeb6b1134ab8638530.txt'],\n",
       "       ['../data/corpus/5984cb0ab6b1130b9863852c.txt',\n",
       "        '../data/corpus/5984d818b6b1130a266384f1.txt',\n",
       "        '../data/corpus/5984dbd1b6b113237863853f.txt'],\n",
       "       ['../data/corpus/5984c861b6b1136d74638511.txt',\n",
       "        '../data/corpus/5984c20db6b1132c8e6384f7.txt',\n",
       "        '../data/corpus/5984bb6db6b11359be638527.txt']],\n",
       "      dtype='<U43')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-3859bf48b334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mexample_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minit_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-95f8b0e31b65>\u001b[0m in \u001b[0;36minput_pipeline\u001b[0;34m(filenames, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minput_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     filename_queue = tf.train.string_input_producer(\n\u001b[0;32m---> 24\u001b[0;31m       filenames, num_epochs=num_epochs, shuffle=False, seed=0)\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_input_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/input.py\u001b[0m in \u001b[0;36mstring_input_producer\u001b[0;34m(string_tensor, num_epochs, shuffle, seed, capacity, shared_name, name, cancel_op)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \"\"\"\n\u001b[1;32m    215\u001b[0m   \u001b[0mnot_null_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"string_input_producer requires a non-null input tensor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstring_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_null_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():  \n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        example_batch, label_batch = input_pipeline(filenames, batch_size=2, num_epochs=1)        \n",
    "        \n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "                \n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                batches, keys = sess.run([example_batch, label_batch])  \n",
    "#                 time_distributed(batches, conv_2d, [num_filters, filter_sizes, strides])\n",
    "                pprint(batches.shape)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)        \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My input func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7f75ba931550>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'dequeue_up_to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-6f11ad2edfb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0minit_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-6f11ad2edfb8>\u001b[0m in \u001b[0;36minput_pipeline\u001b[0;34m(filenames, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Create a training graph that starts by dequeuing a batch of examples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequeue_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#     train_op = ...use 'inputs' to build the training part of the graph...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'dequeue_up_to'"
     ]
    }
   ],
   "source": [
    "num_threads=cpu_count\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):    \n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=False, seed=0)\n",
    "    example_queue, key = read_input_file(filename_queue)    \n",
    "\n",
    "    # Create a training graph that starts by dequeuing a batch of examples.\n",
    "    inputs = example_queue.dequeue_up_to(batch_size)\n",
    "#     train_op = ...use 'inputs' to build the training part of the graph...\n",
    "\n",
    "    \n",
    "#     min_after_dequeue = 10000\n",
    "#     capacity = min_after_dequeue + 3 * batch_size\n",
    "#     example_batch, label_batch = tf.train.batch(\n",
    "#         [example, key], \n",
    "#         batch_size=batch_size, \n",
    "#         capacity=capacity,\n",
    "# #         min_after_dequeue=min_after_dequeue,\n",
    "#         dynamic_pad=True,\n",
    "#         allow_smaller_final_batch=True,\n",
    "#         num_threads=cpu_count\n",
    "#     )\n",
    "#     return example_batch, label_batch\n",
    "    return inputs\n",
    "\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():  \n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        inputs = input_pipeline(filenames, batch_size=2, num_epochs=1)        \n",
    "        \n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "                \n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                examples = sess.run([inputs, label_batch])  \n",
    "#                 time_distributed(batches, conv_2d, [num_filters, filter_sizes, strides])\n",
    "                pprint(keys)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
