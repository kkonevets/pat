{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:33.610902Z",
     "start_time": "2017-07-20T11:58:33.580424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from common import *\n",
    "from glob import glob\n",
    "from os import rename, path\n",
    "from gensim import corpora\n",
    "from os.path import basename\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pickle, json\n",
    "from numba import jit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import multiprocessing\n",
    "import copy, logging\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count() -1\n",
    "\n",
    "DATA_FOLDER = '../data/'\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T12:01:59.806459Z",
     "start_time": "2017-07-20T12:01:59.521851Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_docs = get_all_docs(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:15.293715Z",
     "start_time": "2017-07-20T11:58:13.656390Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 14:58:13,665 : INFO : loading Dictionary object from ../data/pure.dict\n",
      "2017-07-20 14:58:15,152 : INFO : loaded ../data/pure.dict\n",
      "2017-07-20 14:58:15,289 : INFO : loaded corpus index from ../data/pure_corpus.mm.index\n",
      "2017-07-20 14:58:15,290 : INFO : initializing corpus reader from ../data/pure_corpus.mm\n",
      "2017-07-20 14:58:15,291 : INFO : accepted corpus with 1194429 documents, 2316883 features, 195209119 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary.load(join(DATA_FOLDER, 'pure.dict'))\n",
    "corpus = corpora.MmCorpus(join(DATA_FOLDER, 'pure_corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:15.298391Z",
     "start_time": "2017-07-20T11:58:15.295304Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(1194429 documents, 2316883 features, 195209119 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:15.817922Z",
     "start_time": "2017-07-20T11:58:15.299949Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 14:58:15,311 : INFO : loading TfidfModel object from ../data/tfidf_pure.model\n",
      "2017-07-20 14:58:15,815 : INFO : loaded ../data/tfidf_pure.model\n"
     ]
    }
   ],
   "source": [
    "fmodel = join(DATA_FOLDER, 'tfidf_pure.model')\n",
    "if not path.exists(fmodel):    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    tfidf.save(fmodel)\n",
    "else:\n",
    "    tfidf = models.TfidfModel.load(fmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:21.526652Z",
     "start_time": "2017-07-20T11:58:15.819514Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 14:58:15,820 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_new_300\n",
      "2017-07-20 14:58:17,351 : INFO : loading wv recursively from ../data/vectors/w2v_model_new_300.wv.* with mmap=None\n",
      "2017-07-20 14:58:17,352 : INFO : loading syn0 from ../data/vectors/w2v_model_new_300.wv.syn0.npy with mmap=None\n",
      "2017-07-20 14:58:18,676 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-07-20 14:58:18,677 : INFO : loading syn1neg from ../data/vectors/w2v_model_new_300.syn1neg.npy with mmap=None\n",
      "2017-07-20 14:58:20,091 : INFO : setting ignored attribute cum_table to None\n",
      "2017-07-20 14:58:20,091 : INFO : loaded ../data/vectors/w2v_model_new_300\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_new_300'))\n",
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T23:38:36.967384Z",
     "start_time": "2017-07-17T23:38:36.965313Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wv.most_similar(positive=['стена'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:36.912148Z",
     "start_time": "2017-07-20T11:58:36.904282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'gold.json'), 'r') as f:\n",
    "    gold = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T11:58:49.576306Z",
     "start_time": "2017-07-20T11:58:49.572354Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_docs = glob(join(DATA_FOLDER, 'validate/*.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T12:25:06.454219Z",
     "start_time": "2017-07-20T12:25:06.448510Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NWORDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T12:25:17.215967Z",
     "start_time": "2017-07-20T12:25:17.205140Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def like(wv, what):\n",
    "    \"\"\"\n",
    "    get first in the list with given pos tag\n",
    "    \"\"\"\n",
    "    pos = ('_NOUN', '_VERB', '_ADJ', '_ADV')\n",
    "    pos = ('_S', '_V', '_A', '_ADV')\n",
    "    return next((wv[what + p] for p in pos if what + p in wv), None)\n",
    "\n",
    "\n",
    "def like2(wv, what):\n",
    "    try:\n",
    "        return wv[what]\n",
    "    except: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T12:25:17.226272Z",
     "start_time": "2017-07-20T12:25:17.217914Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def tovec(doc_tfidf, wv, dictionary):\n",
    "    sorted_tfidf = sorted(doc_tfidf, key=itemgetter(1), reverse=True)\n",
    "    for k,v in sorted_tfidf:\n",
    "        pair = (like2(wv, dictionary.get(k)), v)\n",
    "        if pair[0] is not None:\n",
    "            yield pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T12:25:17.247653Z",
     "start_time": "2017-07-20T12:25:17.227793Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_vec(doc_tfidf, wv, nwords=10):\n",
    "    # take top 10 most meaningfull\n",
    "    valid_tokens = tovec(doc_tfidf, wv, dictionary)\n",
    "    vec_score = np.array([x for x in islice(valid_tokens, nwords)])\n",
    "    if len(vec_score) == 0:\n",
    "        return np.zeros(wv.syn0.shape[1])\n",
    "    docvec = np.mean(vec_score[:, 0] * softmax(vec_score[:, 1]), axis=0) \n",
    "\n",
    "    return docvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T12:25:17.929556Z",
     "start_time": "2017-07-20T12:25:17.252214Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 199.19it/s]\n"
     ]
    }
   ],
   "source": [
    "val_vecs = []\n",
    "val_names = []\n",
    "for fname in tqdm(val_docs):\n",
    "    with open(fname, 'r') as f:\n",
    "        doc_text = f.read()\n",
    "    doc_bow = dictionary.doc2bow(tokenize(doc_text))\n",
    "    docvec = doc_vec(tfidf[doc_bow], wv, NWORDS)\n",
    "    name = path.splitext(basename(fname))[0]\n",
    "    val_names.append(name)\n",
    "    val_vecs.append(docvec)\n",
    "\n",
    "# test_vecs = pd.DataFrame.from_dict(test_vecs, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:17.218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:17.223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 34734/1194429 [00:45<24:21, 793.36it/s]"
     ]
    }
   ],
   "source": [
    "train_vecs = []\n",
    "train_names = []\n",
    "for key, doc_tfidf in enumerate(tqdm(tfidf[corpus])):\n",
    "    docvec = doc_vec(doc_tfidf, wv, NWORDS)\n",
    "    fname = all_docs[key]\n",
    "    name = path.splitext(basename(fname))[0]\n",
    "    train_names.append(name)\n",
    "    train_vecs.append(docvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:17.226Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:17.229Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p {join(DATA_FOLDER, 'saved/')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:17.247Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/train.pkl'), 'wb') as f:\n",
    "    pickle.dump([train_vecs, train_names], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:17.260Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/val.pkl'), 'wb') as f:\n",
    "    pickle.dump([val_vecs, val_names], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T12:00:19.662005Z",
     "start_time": "2017-07-19T12:00:15.643930Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/train.pkl'), 'rb') as f:\n",
    "    train_vecs, train_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T12:00:19.669494Z",
     "start_time": "2017-07-19T12:00:19.663274Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/val.pkl'), 'rb') as f:\n",
    "    val_vecs, val_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-13T08:09:52.543899Z",
     "start_time": "2017-07-13T08:09:52.541671Z"
    }
   },
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:20.790Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(val_vecs, train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:20.795Z"
    }
   },
   "outputs": [],
   "source": [
    "best = {}\n",
    "for i, vec in enumerate(tqdm(sim_mat)):\n",
    "    val_name = val_names[i]\n",
    "    train_ixs = vec.argsort()[-200:][::-1]\n",
    "    top_train = [train_names[i] for i in train_ixs]\n",
    "    best[val_name] = top_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-20T12:25:30.418Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = evaluate(best, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. nword = 10\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.2\n",
    "acc200    0.5\n",
    "mean\n",
    "acc10     0.360327\n",
    "acc20     0.420654\n",
    "acc200    0.510225\n",
    "\n",
    "2.nword = 10, softmax\n",
    "median\n",
    "acc10     0.000000\n",
    "acc20     0.333333\n",
    "acc200    0.500000\n",
    "mean\n",
    "acc10     0.385890\n",
    "acc20     0.442331\n",
    "acc200    0.549080\n",
    "\n",
    "3. nword=100, softmax\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.0\n",
    "acc200    0.5\n",
    "mean\n",
    "acc10     0.329243\n",
    "acc20     0.387526\n",
    "acc200    0.540389\n",
    "\n",
    "4. nword=10, dim=300, average simple\n",
    "median\n",
    "acc10     0.5\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.519632\n",
    "acc20     0.610123\n",
    "acc200    0.832720\n",
    "dtype: float64\n",
    "\n",
    "5. nword=10, dim=300, tfidf weights\n",
    "median\n",
    "acc10     0.5\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.528834\n",
    "acc20     0.625869\n",
    "acc200    0.838855\n",
    "6.nword=20, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.640286\n",
    "acc20     0.689571\n",
    "acc200    0.871166\n",
    "dtype: float64\n",
    "7.nword=100, dim=300, tfidf weights\n",
    "median\n",
    "acc10     0.5\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.523926\n",
    "acc20     0.580164\n",
    "acc200    0.729857\n",
    "dtype: float64\n",
    "8.nword=30, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.602863\n",
    "acc20     0.663599\n",
    "acc200    0.832106\n",
    "9.nword=15, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.633742\n",
    "acc20     0.728221\n",
    "acc200    0.884458\n",
    "10.nword=5, dim=300, tfidf weights\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.321166\n",
    "acc20     0.369427\n",
    "acc200    0.612577\n",
    "11.nword=13, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.605930\n",
    "acc20     0.676687\n",
    "acc200    0.840286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T22:51:59.061906Z",
     "start_time": "2017-07-05T22:51:59.058839Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_vecs = np.array(test_vecs)\n",
    "train_vecs = np.array(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T22:51:59.086742Z",
     "start_time": "2017-07-05T22:51:59.063058Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# np.save(join(DATA_FOLDER, 'bov_test_vecs.npy'), test_vecs)\n",
    "# np.save(join(DATA_FOLDER, 'bov_train_vecs.npy'), train_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensorboard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:53.611780Z",
     "start_time": "2017-07-09T10:10:53.593138Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from operator import itemgetter \n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:54.784746Z",
     "start_time": "2017-07-09T10:10:53.658264Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# subsample to managebale size\n",
    "samp_ixs = sample(range(len(train_vecs)), 99800)\n",
    "samp = list(itemgetter(*samp_ixs)(train_vecs)) + test_vecs\n",
    "samp_names = list(itemgetter(*samp_ixs)(train_names)) + test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:54.841719Z",
     "start_time": "2017-07-09T10:10:54.786909Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(samp)\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:56.484505Z",
     "start_time": "2017-07-09T10:10:54.843816Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "vocab = dict(enumerate(samp_names))\n",
    "vdf = pd.DataFrame.from_dict(vocab, orient='index')\n",
    "vdf.to_csv('../data/processed_docs/vocab.tsv', header=False, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:14:25.908441Z",
     "start_time": "2017-07-09T10:10:56.486617Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    final_embed_matrix = sess.run(embedding_init, feed_dict={embedding_placeholder: samp})\n",
    "    \n",
    "#     final_embed_matrix = sess.run(weights)\n",
    "\n",
    "    # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "    embedding_var = tf.Variable(final_embed_matrix, name='documents')\n",
    "    sess.run(embedding_var.initializer)\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    summary_writer = tf.summary.FileWriter('../data/processed_docs')\n",
    "\n",
    "    # add embedding to the config file\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "\n",
    "    # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "    embedding.metadata_path = 'vocab.tsv'\n",
    "\n",
    "    # saves a configuration file that TensorBoard will read during startup.\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    saver_embed = tf.train.Saver([embedding_var])\n",
    "    saver_embed.save(sess, '../data/processed_docs/model3.ckpt', 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
