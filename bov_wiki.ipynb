{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:14.445017Z",
     "start_time": "2017-07-21T22:18:55.621465Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from common import *\n",
    "from glob import glob\n",
    "from os import rename, path\n",
    "from gensim import corpora\n",
    "from os.path import basename\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import islice\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pickle, json\n",
    "from numba import jit\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import multiprocessing\n",
    "import copy, logging\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count() -1\n",
    "\n",
    "DATA_FOLDER = '../data/'\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:14.677864Z",
     "start_time": "2017-07-21T22:19:14.446282Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_docs = get_all_docs(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:16.189777Z",
     "start_time": "2017-07-21T22:19:14.679518Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-22 01:19:14,681 : INFO : loading Dictionary object from ../data/pure.dict\n",
      "2017-07-22 01:19:16,051 : INFO : loaded ../data/pure.dict\n",
      "2017-07-22 01:19:16,186 : INFO : loaded corpus index from ../data/pure_corpus.mm.index\n",
      "2017-07-22 01:19:16,186 : INFO : initializing corpus reader from ../data/pure_corpus.mm\n",
      "2017-07-22 01:19:16,187 : INFO : accepted corpus with 1194429 documents, 2316883 features, 195209119 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary.load(join(DATA_FOLDER, 'pure.dict'))\n",
    "corpus = corpora.MmCorpus(join(DATA_FOLDER, 'pure_corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:16.194307Z",
     "start_time": "2017-07-21T22:19:16.191299Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(1194429 documents, 2316883 features, 195209119 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:16.624569Z",
     "start_time": "2017-07-21T22:19:16.195575Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-22 01:19:16,202 : INFO : loading TfidfModel object from ../data/tfidf_pure.model\n",
      "2017-07-22 01:19:16,622 : INFO : loaded ../data/tfidf_pure.model\n"
     ]
    }
   ],
   "source": [
    "fmodel = join(DATA_FOLDER, 'tfidf_pure.model')\n",
    "if not path.exists(fmodel):    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    tfidf.save(fmodel)\n",
    "else:\n",
    "    tfidf = models.TfidfModel.load(fmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.034440Z",
     "start_time": "2017-07-21T22:19:16.626009Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-22 01:19:16,627 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-07-22 01:19:18,659 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-07-22 01:19:18,660 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-07-22 01:19:18,942 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-07-22 01:19:18,943 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-07-22 01:19:19,270 : INFO : setting ignored attribute cum_table to None\n",
      "2017-07-22 01:19:19,271 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))\n",
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.038009Z",
     "start_time": "2017-07-21T22:19:21.035896Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wv.most_similar(positive=['стена'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.051007Z",
     "start_time": "2017-07-21T22:19:21.039375Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'gold.json'), 'r') as f:\n",
    "    gold = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.061712Z",
     "start_time": "2017-07-21T22:19:21.052239Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_docs = glob(join(DATA_FOLDER, 'validate/*.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.074367Z",
     "start_time": "2017-07-21T22:19:21.063041Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NWORDS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.146455Z",
     "start_time": "2017-07-21T22:19:21.075737Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def word_score(doc_tfidf, wv, dictionary):\n",
    "    sorted_tfidf = sorted(doc_tfidf, key=itemgetter(1), reverse=True)\n",
    "    for k,score in sorted_tfidf:\n",
    "        word = dictionary.get(k)\n",
    "        if word in wv:\n",
    "            yield (word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.159163Z",
     "start_time": "2017-07-21T22:19:21.147833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_vec(doc_tfidf, wv, nwords=10):\n",
    "    # take top 10 most meaningfull\n",
    "    scored = word_score(doc_tfidf, wv, dictionary)    \n",
    "    ws = list(zip(*islice(scored, nwords)))\n",
    "        \n",
    "    if ws is None:\n",
    "        return np.zeros(wv.syn0.shape[1])\n",
    "    \n",
    "    words, scores = ws\n",
    "    vecs = wv[words]    \n",
    "    docvec = np.mean(vecs.T * softmax(scores), axis=1) \n",
    "\n",
    "    return docvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:19:21.936565Z",
     "start_time": "2017-07-21T22:19:21.160569Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:00<00:00, 171.86it/s]\n"
     ]
    }
   ],
   "source": [
    "val_vecs = []\n",
    "val_names = []\n",
    "for fname in tqdm(val_docs):\n",
    "    with open(fname, 'r') as f:\n",
    "        doc_text = f.read()\n",
    "    doc_bow = dictionary.doc2bow(tokenize(doc_text))\n",
    "    docvec = doc_vec(tfidf[doc_bow], wv, NWORDS)\n",
    "    name = path.splitext(basename(fname))[0]\n",
    "    val_names.append(name)\n",
    "    val_vecs.append(docvec)\n",
    "\n",
    "# test_vecs = pd.DataFrame.from_dict(test_vecs, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:06.590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:06.597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 65617/1194429 [01:28<24:56, 754.34it/s]"
     ]
    }
   ],
   "source": [
    "train_vecs = []\n",
    "train_names = []\n",
    "for key, doc_tfidf in enumerate(tqdm(tfidf[corpus])):\n",
    "    docvec = doc_vec(doc_tfidf, wv, NWORDS)\n",
    "    fname = all_docs[key]\n",
    "    name = path.splitext(basename(fname))[0]\n",
    "    train_names.append(name)\n",
    "    train_vecs.append(docvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:06.599Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:06.601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p {join(DATA_FOLDER, 'saved/')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:06.603Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/train.pkl'), 'wb') as f:\n",
    "    pickle.dump([train_vecs, train_names], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:06.604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/val.pkl'), 'wb') as f:\n",
    "    pickle.dump([val_vecs, val_names], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T13:00:33.497988Z",
     "start_time": "2017-07-20T13:00:33.446630Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/train.pkl'), 'rb') as f:\n",
    "    train_vecs, train_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T13:00:33.502917Z",
     "start_time": "2017-07-20T13:00:33.499464Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'saved/val.pkl'), 'rb') as f:\n",
    "    val_vecs, val_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-13T08:09:52.543899Z",
     "start_time": "2017-07-13T08:09:52.541671Z"
    }
   },
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:09.470Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(val_vecs, train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:09.472Z"
    }
   },
   "outputs": [],
   "source": [
    "best = {}\n",
    "for i, vec in enumerate(tqdm(sim_mat)):\n",
    "    val_name = val_names[i]\n",
    "    train_ixs = vec.argsort()[-200:][::-1]\n",
    "    top_train = [train_names[i] for i in train_ixs]\n",
    "    best[val_name] = top_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:09.474Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'best.json'), 'w') as f:\n",
    "    json.dump(best, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T22:19:09.476Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = evaluate(best, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "different vectors\n",
    "1. nword15, 300, 3\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.0\n",
    "acc200    0.5\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.297436\n",
    "acc20     0.384872\n",
    "acc200    0.548846\n",
    "\n",
    "2. \n",
    "\n",
    "\n",
    "1. nword = 10\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.2\n",
    "acc200    0.5\n",
    "mean\n",
    "acc10     0.360327\n",
    "acc20     0.420654\n",
    "acc200    0.510225\n",
    "\n",
    "2.nword = 10, softmax\n",
    "median\n",
    "acc10     0.000000\n",
    "acc20     0.333333\n",
    "acc200    0.500000\n",
    "mean\n",
    "acc10     0.385890\n",
    "acc20     0.442331\n",
    "acc200    0.549080\n",
    "\n",
    "3. nword=100, softmax\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.0\n",
    "acc200    0.5\n",
    "mean\n",
    "acc10     0.329243\n",
    "acc20     0.387526\n",
    "acc200    0.540389\n",
    "\n",
    "4. nword=10, dim=300, average simple\n",
    "median\n",
    "acc10     0.5\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.519632\n",
    "acc20     0.610123\n",
    "acc200    0.832720\n",
    "dtype: float64\n",
    "\n",
    "5. nword=10, dim=300, tfidf weights\n",
    "median\n",
    "acc10     0.5\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.528834\n",
    "acc20     0.625869\n",
    "acc200    0.838855\n",
    "6.nword=20, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.640286\n",
    "acc20     0.689571\n",
    "acc200    0.871166\n",
    "dtype: float64\n",
    "7.nword=100, dim=300, tfidf weights\n",
    "median\n",
    "acc10     0.5\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.523926\n",
    "acc20     0.580164\n",
    "acc200    0.729857\n",
    "dtype: float64\n",
    "8.nword=30, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.602863\n",
    "acc20     0.663599\n",
    "acc200    0.832106\n",
    "9.nword=15, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.633742\n",
    "acc20     0.728221\n",
    "acc200    0.884458\n",
    "10.nword=5, dim=300, tfidf weights\n",
    "median\n",
    "acc10     0.0\n",
    "acc20     0.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.321166\n",
    "acc20     0.369427\n",
    "acc200    0.612577\n",
    "11.nword=13, dim=300, tfidf weights\n",
    "median\n",
    "acc10     1.0\n",
    "acc20     1.0\n",
    "acc200    1.0\n",
    "dtype: float64\n",
    "mean\n",
    "acc10     0.605930\n",
    "acc20     0.676687\n",
    "acc200    0.840286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T22:51:59.061906Z",
     "start_time": "2017-07-05T22:51:59.058839Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_vecs = np.array(test_vecs)\n",
    "train_vecs = np.array(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T22:51:59.086742Z",
     "start_time": "2017-07-05T22:51:59.063058Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# np.save(join(DATA_FOLDER, 'bov_test_vecs.npy'), test_vecs)\n",
    "# np.save(join(DATA_FOLDER, 'bov_train_vecs.npy'), train_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensorboard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:53.611780Z",
     "start_time": "2017-07-09T10:10:53.593138Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from operator import itemgetter \n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:54.784746Z",
     "start_time": "2017-07-09T10:10:53.658264Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# subsample to managebale size\n",
    "samp_ixs = sample(range(len(train_vecs)), 99800)\n",
    "samp = list(itemgetter(*samp_ixs)(train_vecs)) + test_vecs\n",
    "samp_names = list(itemgetter(*samp_ixs)(train_names)) + test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:54.841719Z",
     "start_time": "2017-07-09T10:10:54.786909Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(samp)\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:10:56.484505Z",
     "start_time": "2017-07-09T10:10:54.843816Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "vocab = dict(enumerate(samp_names))\n",
    "vdf = pd.DataFrame.from_dict(vocab, orient='index')\n",
    "vdf.to_csv('../data/processed_docs/vocab.tsv', header=False, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-09T10:14:25.908441Z",
     "start_time": "2017-07-09T10:10:56.486617Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    final_embed_matrix = sess.run(embedding_init, feed_dict={embedding_placeholder: samp})\n",
    "    \n",
    "#     final_embed_matrix = sess.run(weights)\n",
    "\n",
    "    # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "    embedding_var = tf.Variable(final_embed_matrix, name='documents')\n",
    "    sess.run(embedding_var.initializer)\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    summary_writer = tf.summary.FileWriter('../data/processed_docs')\n",
    "\n",
    "    # add embedding to the config file\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "\n",
    "    # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "    embedding.metadata_path = 'vocab.tsv'\n",
    "\n",
    "    # saves a configuration file that TensorBoard will read during startup.\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    saver_embed = tf.train.Saver([embedding_var])\n",
    "    saver_embed.save(sess, '../data/processed_docs/model3.ckpt', 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
