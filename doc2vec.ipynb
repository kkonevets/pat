{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:59:42.676019Z",
     "start_time": "2017-09-30T15:59:37.340068Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "from matplotlib import pyplot as plt\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T15:59:42.698583Z",
     "start_time": "2017-09-30T15:59:42.678819Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'gold_mongo.json'), 'r') as f:\n",
    "    gold = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T14:45:22.763Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_docs(fnames):\n",
    "    for i, fn in enumerate(fnames):\n",
    "#         logging.info(\"%s: \" % i + fn)\n",
    "        with GzipFile(fn, 'rb') as f:\n",
    "            docs = ujson.loads(f.read())\n",
    "        for k,doc in docs.items():\n",
    "            yield k,doc\n",
    "\n",
    "                \n",
    "class Documents(object):\n",
    "    def __init__(self, folder, tokens_only=False):\n",
    "        self.folder = folder\n",
    "        self.tokens_only = tokens_only\n",
    " \n",
    "    def __iter__(self):\n",
    "        fnames = glob(join(self.folder, '*.json.gz'))\n",
    "        for k, doc in iter_docs(fnames):\n",
    "            unlisted = [w for s in doc for w in s]\n",
    "            if self.tokens_only:\n",
    "                yield unlisted\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(unlisted, [k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T14:45:22.768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-30 18:59:42,721 [MainThread  ] [INFO ]  collecting all words and their counts\n",
      "2017-09-30 18:59:42,793 [MainThread  ] [INFO ]  PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-09-30 18:59:48,312 [MainThread  ] [INFO ]  PROGRESS: at example #10000, processed 5214517 words (945145/s), 113415 word types, 10000 tags\n",
      "2017-09-30 18:59:54,478 [MainThread  ] [INFO ]  PROGRESS: at example #20000, processed 10791718 words (904626/s), 157599 word types, 20000 tags\n",
      "2017-09-30 19:00:04,194 [MainThread  ] [INFO ]  PROGRESS: at example #30000, processed 19494360 words (895872/s), 197295 word types, 30000 tags\n",
      "2017-09-30 19:00:16,325 [MainThread  ] [INFO ]  PROGRESS: at example #40000, processed 30923454 words (942234/s), 249001 word types, 40000 tags\n",
      "2017-09-30 19:00:29,758 [MainThread  ] [INFO ]  PROGRESS: at example #50000, processed 42448329 words (858070/s), 299432 word types, 50000 tags\n",
      "2017-09-30 19:00:46,365 [MainThread  ] [INFO ]  PROGRESS: at example #60000, processed 56836755 words (866496/s), 369889 word types, 60000 tags\n",
      "2017-09-30 19:01:05,421 [MainThread  ] [INFO ]  PROGRESS: at example #70000, processed 76509609 words (1032432/s), 421371 word types, 70000 tags\n",
      "2017-09-30 19:01:23,925 [MainThread  ] [INFO ]  PROGRESS: at example #80000, processed 91801743 words (826450/s), 465563 word types, 80000 tags\n",
      "2017-09-30 19:01:32,467 [MainThread  ] [INFO ]  PROGRESS: at example #90000, processed 101352279 words (1118272/s), 512948 word types, 90000 tags\n",
      "2017-09-30 19:01:39,778 [MainThread  ] [INFO ]  PROGRESS: at example #100000, processed 108122533 words (926184/s), 530265 word types, 100000 tags\n",
      "2017-09-30 19:01:44,693 [MainThread  ] [INFO ]  PROGRESS: at example #110000, processed 112176238 words (825014/s), 539094 word types, 110000 tags\n",
      "2017-09-30 19:01:59,732 [MainThread  ] [INFO ]  PROGRESS: at example #120000, processed 126237802 words (935077/s), 584270 word types, 120000 tags\n",
      "2017-09-30 19:02:11,375 [MainThread  ] [INFO ]  PROGRESS: at example #130000, processed 136499002 words (881435/s), 612118 word types, 130000 tags\n",
      "2017-09-30 19:02:25,267 [MainThread  ] [INFO ]  PROGRESS: at example #140000, processed 149483371 words (934742/s), 640503 word types, 140000 tags\n",
      "2017-09-30 19:02:30,402 [MainThread  ] [INFO ]  PROGRESS: at example #150000, processed 154117112 words (902507/s), 672652 word types, 150000 tags\n",
      "2017-09-30 19:02:41,981 [MainThread  ] [INFO ]  PROGRESS: at example #160000, processed 163690305 words (826848/s), 697511 word types, 160000 tags\n",
      "2017-09-30 19:02:51,208 [MainThread  ] [INFO ]  PROGRESS: at example #170000, processed 172348513 words (938519/s), 713097 word types, 170000 tags\n",
      "2017-09-30 19:03:01,842 [MainThread  ] [INFO ]  PROGRESS: at example #180000, processed 181331023 words (844835/s), 734041 word types, 180000 tags\n",
      "2017-09-30 19:03:16,777 [MainThread  ] [INFO ]  PROGRESS: at example #190000, processed 195526545 words (950567/s), 785989 word types, 190000 tags\n",
      "2017-09-30 19:03:33,880 [MainThread  ] [INFO ]  PROGRESS: at example #200000, processed 210573401 words (879856/s), 815620 word types, 200000 tags\n",
      "2017-09-30 19:03:53,846 [MainThread  ] [INFO ]  PROGRESS: at example #210000, processed 228941922 words (920048/s), 848705 word types, 210000 tags\n",
      "2017-09-30 19:04:03,121 [MainThread  ] [INFO ]  PROGRESS: at example #220000, processed 236957506 words (864319/s), 864760 word types, 220000 tags\n",
      "2017-09-30 19:04:12,655 [MainThread  ] [INFO ]  PROGRESS: at example #230000, processed 245334525 words (878744/s), 881891 word types, 230000 tags\n",
      "2017-09-30 19:04:26,496 [MainThread  ] [INFO ]  PROGRESS: at example #240000, processed 257462429 words (876369/s), 906130 word types, 240000 tags\n",
      "2017-09-30 19:04:37,365 [MainThread  ] [INFO ]  PROGRESS: at example #250000, processed 266703163 words (850282/s), 923792 word types, 250000 tags\n",
      "2017-09-30 19:04:50,545 [MainThread  ] [INFO ]  PROGRESS: at example #260000, processed 278559175 words (899645/s), 967533 word types, 260000 tags\n",
      "2017-09-30 19:05:00,182 [MainThread  ] [INFO ]  PROGRESS: at example #270000, processed 286941125 words (869792/s), 980349 word types, 270000 tags\n",
      "2017-09-30 19:05:11,370 [MainThread  ] [INFO ]  PROGRESS: at example #280000, processed 296682189 words (870820/s), 1043801 word types, 280000 tags\n",
      "2017-09-30 19:05:29,444 [MainThread  ] [INFO ]  PROGRESS: at example #290000, processed 313000657 words (902941/s), 1069506 word types, 290000 tags\n",
      "2017-09-30 19:05:47,722 [MainThread  ] [INFO ]  PROGRESS: at example #300000, processed 329173333 words (884870/s), 1094939 word types, 300000 tags\n",
      "2017-09-30 19:06:00,498 [MainThread  ] [INFO ]  PROGRESS: at example #310000, processed 340373020 words (876680/s), 1115731 word types, 310000 tags\n",
      "2017-09-30 19:06:18,224 [MainThread  ] [INFO ]  PROGRESS: at example #320000, processed 355218833 words (837560/s), 1173909 word types, 320000 tags\n",
      "2017-09-30 19:06:32,418 [MainThread  ] [INFO ]  PROGRESS: at example #330000, processed 368178378 words (913141/s), 1209667 word types, 330000 tags\n",
      "2017-09-30 19:06:47,032 [MainThread  ] [INFO ]  PROGRESS: at example #340000, processed 379308417 words (761665/s), 1242545 word types, 340000 tags\n",
      "2017-09-30 19:07:06,814 [MainThread  ] [INFO ]  PROGRESS: at example #350000, processed 397405582 words (914877/s), 1270880 word types, 350000 tags\n",
      "2017-09-30 19:07:21,747 [MainThread  ] [INFO ]  PROGRESS: at example #360000, processed 412573454 words (1015871/s), 1291069 word types, 360000 tags\n",
      "2017-09-30 19:07:35,320 [MainThread  ] [INFO ]  PROGRESS: at example #370000, processed 422609080 words (739425/s), 1305805 word types, 370000 tags\n",
      "2017-09-30 19:07:47,697 [MainThread  ] [INFO ]  PROGRESS: at example #380000, processed 435448613 words (1037476/s), 1323179 word types, 380000 tags\n",
      "2017-09-30 19:08:03,599 [MainThread  ] [INFO ]  PROGRESS: at example #390000, processed 449037241 words (854596/s), 1350852 word types, 390000 tags\n",
      "2017-09-30 19:08:14,590 [MainThread  ] [INFO ]  PROGRESS: at example #400000, processed 458116888 words (826185/s), 1422999 word types, 400000 tags\n"
     ]
    }
   ],
   "source": [
    "folder = join(DATA_FOLDER, 'documents/')\n",
    "\n",
    "model = Doc2Vec(Documents(folder), size=200, window=15, min_count=5, dm=1, workers=cpu_count, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T14:45:22.769Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('../data/saved/d2v_model_s200_w15_mc5_dm1_iter10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T04:52:17.514623Z",
     "start_time": "2017-09-30T04:52:17.455129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нитрозативный, 0.328898489475\n",
      "мамматон, 0.315056562424\n",
      "деметоксилировать, 0.310829102993\n",
      "теносклеропластика, 0.309251964092\n",
      "пискаревский, 0.30451887846\n",
      "технологичность, 0.302434027195\n",
      "ddip, 0.299423635006\n",
      "перекомпиляция, 0.294496178627\n",
      "диметоксикоричный, 0.294087588787\n",
      "mulchandani, 0.292344450951\n"
     ]
    }
   ],
   "source": [
    "most = model.most_similar(u'стол')\n",
    "for w, score in most:\n",
    "    print(w + ', %s' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T14:45:29.495Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_docs = {}\n",
    "fnames = glob(join(folder, '*.json.gz'))\n",
    "for k,doc in iter_docs(fnames):\n",
    "    if k in gold.keys():\n",
    "        unlisted = [w for s in doc for w in s]\n",
    "        test_docs[k] = unlisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T14:45:29.497Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for k, doc in tqdm_notebook(test_docs.items()):\n",
    "    inferred_vector = model.infer_vector(doc, steps=5)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=200)\n",
    "    preds[k] = [i for i,score in sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T14:45:29.499Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = evaluate(preds, gold)\n",
    "\n",
    "ax = result['acc10'].hist()\n",
    "ax.set_xlabel(\"acc10\")\n",
    "plt.show()\n",
    "\n",
    "ax = result['acc20'].hist()\n",
    "ax.set_xlabel(\"acc20\")\n",
    "plt.show()\n",
    "\n",
    "ax = result['acc200'].hist()\n",
    "ax.set_xlabel(\"acc200\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
