{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from itertools import islice\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_docs = glob.iglob(os.path.join(DATA_FOLDER, 'FIPS') + '/**/*.txt', recursive=True)\n",
    "# corpus_file = open(DATA_FOLDER + 'corpus.txt', 'w')\n",
    "# for item in all_docs:\n",
    "#     corpus_file.write(\"%s\\n\" % os.path.relpath(item, DATA_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize.toktok import ToktokTokenizer \n",
    "# toktok = ToktokTokenizer()\n",
    "# toktok.tokenize('устройство по п . 3 , отличать тем , что оно снабжать устанавливать в головка винт , в который размещать шарик и ролик .  . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = stopwords.words('russian')\n",
    "stop_list.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на', 'ко'])\n",
    "stop_list.extend(list(string.punctuation))\n",
    "\n",
    "punkts = [s for s in string.punctuation if s not in '.!?']\n",
    "\n",
    "\n",
    "class Corpus(gensim.corpora.TextCorpus):\n",
    "    def get_texts(self):\n",
    "        for filename in tqdm(self.input): # for each relevant file\n",
    "            yield tokenize(open(filename).read())\n",
    "\n",
    "            \n",
    "def save_corpus(list_block, dir_name, prefix):\n",
    "    corpus = Corpus(list_block)\n",
    "    \n",
    "    dic_name = os.path.join(dir_name, '%s.dict' % prefix)\n",
    "    corp_name = os.path.join(dir_name, '%s_corpus.mm' % prefix)\n",
    "    \n",
    "    corpus.dictionary.save(dic_name)\n",
    "    corpora.MmCorpus.serialize(corp_name, corpus)\n",
    "    \n",
    "    return dic_name, corp_name\n",
    "    \n",
    "            \n",
    "def tokenize(file_text):\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    tokens = [validate_word(word) for word in tokens if check_word(word)]\n",
    "    tokens = filter(lambda s: s != '', tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "            \n",
    "def validate_word(word):\n",
    "    word = word.replace(\"«\", \"\").replace(\"»\", \"\").replace(\"`\", '').replace(\".\", '')\n",
    "    if len(word) == 1:\n",
    "        return ''\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def check_word(word):\n",
    "    if word in stop_list:\n",
    "        return False\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grouper(n, iterable):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "       chunk = tuple(islice(it, n))\n",
    "       if not chunk:\n",
    "           return\n",
    "       yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11944/11944 [09:27<00:00, 21.03it/s]\n",
      "100%|██████████| 11944/11944 [09:37<00:00, 29.62it/s]\n",
      "100%|██████████| 11944/11944 [09:39<00:00, 20.62it/s]\n",
      "100%|██████████| 11944/11944 [09:44<00:00, 20.45it/s]\n",
      "100%|██████████| 11944/11944 [09:38<00:00, 20.65it/s]\n",
      "100%|██████████| 11944/11944 [09:42<00:00, 20.49it/s]\n",
      "100%|██████████| 11944/11944 [09:43<00:00, 20.47it/s]\n",
      "  0%|          | 16/11944 [00:01<18:13, 10.91it/s]]s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dfolder = os.path.join(DATA_FOLDER, 'FIPS/Inventions patents_txt_output/0c/0c')\n",
    "# all_docs = glob.glob(dfolder + '/**/*.txt', recursive=True)\n",
    "\n",
    "all_docs = glob.glob(os.path.join(DATA_FOLDER, 'FIPS') + '/**/*.txt', recursive=True)\n",
    "\n",
    "parallelizer = Parallel(n_jobs=cpu_count)\n",
    "\n",
    "# this iterator returns the functions to execute for each task\n",
    "tasks_iterator = ( delayed(save_corpus)(list_block, os.path.join(DATA_FOLDER, 'BoW'), i) for \n",
    "                  i, list_block in enumerate(grouper(len(all_docs)//100, all_docs)) ) \n",
    "result = parallelizer( tasks_iterator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_corpus = None\n",
    "for dic_name, corp_name in result:\n",
    "    cur_dict = corpora.Dictionary.load(dic_name)\n",
    "    cur_corp = corpora.MmCorpus(corp_name)\n",
    "    \n",
    "    if not merged_corpus:\n",
    "        dict1 = cur_dict\n",
    "        merged_corpus = cur_corp\n",
    "        continue\n",
    "\n",
    "    cur_to_dict1 = dict1.merge_with(cur_dict)\n",
    "    merged_corpus = itertools.chain(merged_corpus, cur_to_dict1[cur_corp])\n",
    "\n",
    "dict1.save(os.path.join(DATA_FOLDER, 'dict.dict'))\n",
    "corpora.MmCorpus.serialize(os.path.join(DATA_FOLDER, 'corpus.mm'), merged_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
