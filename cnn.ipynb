{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:02:30.920274Z",
     "start_time": "2017-08-24T19:02:27.999876Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "import tflearn.helpers.summarizer as s\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:09.806071Z",
     "start_time": "2017-08-23T10:20:03.352690Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-23 10:20:03,354 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-23 10:20:06,546 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-23 10:20:06,548 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-23 10:20:06,849 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-23 10:20:06,851 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-23 10:20:07,159 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-23 10:20:07,160 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:12.062139Z",
     "start_time": "2017-08-23T10:20:09.808583Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:16.821848Z",
     "start_time": "2017-08-23T10:20:12.064429Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.849548Z",
     "start_time": "2017-08-23T10:20:16.824501Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193705\n",
       "std           0.070574\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.857777Z",
     "start_time": "2017-08-23T10:20:54.851863Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18944918087888768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.249141Z",
     "start_time": "2017-08-23T10:20:54.859933Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.988410Z",
     "start_time": "2017-08-23T10:20:55.251655Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:02:33.892151Z",
     "start_time": "2017-08-24T19:02:30.927782Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:02:38.377632Z",
     "start_time": "2017-08-24T19:02:33.894164Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:02:38.389462Z",
     "start_time": "2017-08-24T19:02:38.379603Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random select nagative examples\n",
    "import random\n",
    "\n",
    "def full_name(_id):\n",
    "    return join(DATA_FOLDER, 'corpus/%s.txt' % _id)\n",
    "\n",
    "def random_triples(sims, ids, num_epochs=1, seed=0):\n",
    "    \"\"\"\n",
    "    Get random triples, select negatives at random in each epoch.\n",
    "    Output: [anchor, positive, negative]\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    ixs = list(range(len(ids)))\n",
    "    for ep in range(num_epochs):\n",
    "        random.shuffle(ixs)\n",
    "        it = iter(ixs)\n",
    "        for k, v in tqdm(sims.items()):\n",
    "            exclude = [full_name(i) for i in [k] + v]\n",
    "            for vi in v:\n",
    "                ix = next(it)\n",
    "                _neg = ids[ix]\n",
    "                while _neg in exclude:\n",
    "                    ix = next(it)\n",
    "                    _neg = ids[ix]\n",
    "                yield [full_name(k), full_name(vi), _neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:50:46.904875Z",
     "start_time": "2017-08-24T19:50:46.882595Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    with tf.name_scope('parse_csv'):\n",
    "        strings = tf.string_split([text], delimiter='\\n')\n",
    "        raw_nums = tf.string_split(strings.values)\n",
    "        nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "        dense = tf.sparse_to_dense(\n",
    "            raw_nums.indices, raw_nums.dense_shape, nums, default_value=0)\n",
    "        dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    with tf.name_scope('read_input_tuple'):\n",
    "        fnames = filename_queue.dequeue()\n",
    "        example = []\n",
    "        for fn in tf.unstack(fnames):\n",
    "            record_string = tf.read_file(fn)\n",
    "            arr = parse_csv(record_string)\n",
    "            example.append(arr)\n",
    "    return example\n",
    "\n",
    "def input_pipeline(triples, batch_size, num_epochs=1, num_threads=cpu_count):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        triples, num_epochs=num_epochs, capacity=32, shuffle=True, seed=0)\n",
    "    example = read_input_tuple(filename_queue)\n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    anchor, positive, negative = tf.train.batch(\n",
    "        example,\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        #         allow_smaller_final_batch=True,\n",
    "        num_threads=num_threads)\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T20:24:37.152223Z",
     "start_time": "2017-08-24T20:24:36.911532Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 n_sents,\n",
    "                 n_words,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 sent_filter_sizes=[2, 3, 4, 5],\n",
    "                 sent_nb_filter=15,\n",
    "                 doc_filter_sizes=[1, 2, 3],\n",
    "                 doc_nb_filter=10,\n",
    "                 sent_kmax=10,\n",
    "                 doc_kmax=10,\n",
    "                 learning_rate=0.001):\n",
    "        self.n_sents = n_sents\n",
    "        self.n_words = n_words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sent_filter_sizes = sent_filter_sizes\n",
    "        self.sent_nb_filter = sent_nb_filter\n",
    "        self.doc_filter_sizes = doc_filter_sizes\n",
    "        self.doc_nb_filter = doc_nb_filter\n",
    "        self.sent_kmax = sent_kmax\n",
    "        self.doc_kmax = doc_kmax\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.LT = tf.Variable(\n",
    "                tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                trainable=False,\n",
    "                name=\"W\")\n",
    "\n",
    "            self.embedding_placeholder = tf.placeholder(\n",
    "                tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = self.LT.assign(self.embedding_placeholder)\n",
    "\n",
    "        with tf.variable_scope('sent'):\n",
    "            self._create_sharable_weights(sent_filter_sizes, embedding_size,\n",
    "                                          sent_nb_filter)\n",
    "            self.sent_embedding_size = tf.convert_to_tensor(\n",
    "                sent_kmax * sent_nb_filter * len(sent_filter_sizes))\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            self._create_sharable_weights(doc_filter_sizes,\n",
    "                                          self.sent_embedding_size.eval(),\n",
    "                                          doc_nb_filter)\n",
    "            self.doc_embedding_size = tf.convert_to_tensor(\n",
    "                doc_kmax * doc_nb_filter * len(doc_filter_sizes))\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\" This is the forward calculation from batch X to doc embeddins \"\"\"\n",
    "        \n",
    "        embedded_words = tf.nn.embedding_lookup(self.LT, X)\n",
    "        embedded_words_expanded = tf.expand_dims(embedded_words, -1)\n",
    "        \n",
    "        with tf.variable_scope('sent'):\n",
    "\n",
    "            def convolv_on_sents(embeds):\n",
    "                return self._convolv_on_embeddings(\n",
    "                    embeds, self.sent_filter_sizes, self.sent_nb_filter,\n",
    "                    self.sent_kmax)\n",
    "\n",
    "            # iter over each document\n",
    "            self.sent_embed = tf.map_fn(\n",
    "                convolv_on_sents,\n",
    "                embedded_words_expanded,\n",
    "                parallel_iterations=10,\n",
    "                name='iter_over_docs')\n",
    "            # sent_embed shape is [batch, n_sent, sent_sent_kmax*sent_nb_filter*len(sent_filter_sizes), 1]\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            # finally, convolv on documents\n",
    "            self.doc_embed = self._convolv_on_embeddings(\n",
    "                self.sent_embed, self.doc_filter_sizes, self.doc_nb_filter,\n",
    "                self.doc_kmax)\n",
    "            # doc_embed shape is [batch, doc_kmax*doc_nb_filter*len(doc_filter_sizes), 1]\n",
    "\n",
    "        doc_embed_normalized = tf.nn.l2_normalize(\n",
    "            self.doc_embed, dim=1, name='doc_embed_normalized')\n",
    "\n",
    "        return doc_embed_normalized\n",
    "\n",
    "    def loss(self, X):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            doc_embed_normalized = self.inference(X)\n",
    "            self.anchor, self.positive, self.negative = tf.unstack(\n",
    "                tf.reshape(doc_embed_normalized, [-1, 3, self.doc_embedding_size]),\n",
    "                3, 1)\n",
    "            _loss = self.triplet_loss(self.anchor, self.positive, self.negative)\n",
    "        return _loss\n",
    "\n",
    "    def optimize(self, X):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            self.loss_op = self.loss(X)\n",
    "            self.gradients = self.optimizer.compute_gradients(self.loss_op)\n",
    "            apply_gradient_op = self.optimizer.apply_gradients(\n",
    "                self.gradients, global_step=self.global_step)\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def triplet_loss(self,\n",
    "                     anchor_embed,\n",
    "                     positive_embed,\n",
    "                     negative_embed,\n",
    "                     margin=0.2):\n",
    "        \"\"\"\n",
    "        input: Three L2 normalized tensors of shape [None, dim], compute on a batch\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('triplet_loss'):\n",
    "            d_pos = tf.reduce_sum(tf.square(anchor_embed - positive_embed), 1)\n",
    "            d_neg = tf.reduce_sum(tf.square(anchor_embed - negative_embed), 1)\n",
    "\n",
    "            loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _convolv_on_embeddings(self, embeds, filter_sizes, nb_filter, kmax):\n",
    "        \"\"\"\n",
    "        Create a convolution + k-max pool layer for each filter size, then concat and vectorize.\n",
    "        embeds shape is [batch, (n_words or n_sents), embedding_size, 1]\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for fsize in filter_sizes:\n",
    "            with tf.name_scope(\"conv-%s\" % fsize):\n",
    "                with tf.variable_scope(\n",
    "                        \"conv_weights_fsize-%s\" % fsize, reuse=True):\n",
    "                    weights_init = tf.get_variable('W')\n",
    "                    bias_init = tf.get_variable('b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embeds,\n",
    "                    weights_init,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, bias_init), name=\"relu\")\n",
    "#                 h shape is [batch, n_words - fsize + 1, 1, nb_filter]\n",
    "#             tf.summary.histogram(\"relu\", h)\n",
    "\n",
    "            with tf.name_scope('%s-maxpool-fsize-%s' % (kmax, fsize)):\n",
    "                # k-maxpooling over the outputs\n",
    "                trans = tf.transpose(h, perm=[0, 2, 3, 1])\n",
    "                values, indices = tf.nn.top_k(trans, k=kmax, sorted=False)\n",
    "                pooled = tf.transpose(values, perm=[0, 3, 1, 2])\n",
    "                # pooled shape is [batch, kmax, 1, nb_filter]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        with tf.name_scope('concat_and_vectorize'):\n",
    "            # Combine all the pooled features\n",
    "            h_pool = tf.concat(pooled_outputs, 3)\n",
    "            # h_pool shape is [batch, kmax, 1, nb_filter*len(filter_sizes)]\n",
    "\n",
    "            # Vectorize filters for each sent to get sent embeddings\n",
    "            trans = tf.transpose(h_pool, perm=[0, 2, 3, 1])\n",
    "            batch = tf.shape(embeds)[0]\n",
    "            sent_embed = tf.reshape(trans, [batch, -1, 1])\n",
    "            # sent_embed shape is [batch, kmax*nb_filter*len(filter_sizes), 1]\n",
    "\n",
    "        return sent_embed\n",
    "\n",
    "    def _create_sharable_weights(self, filter_sizes, embedding_size,\n",
    "                                 nb_filter):\n",
    "        \"\"\" Create sharable weights for each type of convolution \"\"\"\n",
    "        with tf.name_scope('sharable_weights'):\n",
    "            for fsize in filter_sizes:\n",
    "                with tf.variable_scope(\"conv_weights_fsize-%s\" % fsize):\n",
    "                    filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                    initializer = tf.contrib.layers.xavier_initializer_conv2d(\n",
    "                        uniform=True)\n",
    "#                     initializer=tf.truncated_normal(stddev=0.1))\n",
    "                    weights_init = tf.get_variable(\n",
    "                        'W', filter_shape, initializer=initializer)\n",
    "                    bias_init = tf.get_variable(\n",
    "                        'b', initializer=tf.constant(0.1, shape=[nb_filter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T20:24:55.759351Z",
     "start_time": "2017-08-24T20:24:42.772285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368458/368458 [00:11<00:00, 30950.99it/s]\n"
     ]
    }
   ],
   "source": [
    "triples_all = list(random_triples(sims, ids, num_epochs=1, seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T20:24:57.472176Z",
     "start_time": "2017-08-24T20:24:55.761652Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_triples, triples_test = train_test_split(triples_all, test_size=0.2, random_state=0)\n",
    "triples_train, triples_val = train_test_split(_triples, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T20:24:57.480797Z",
     "start_time": "2017-08-24T20:24:57.476142Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428414, 183606, 153005)\n"
     ]
    }
   ],
   "source": [
    "print(len(triples_train), len(triples_val), len(triples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-24T20:24:58.343Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_embedding_size 600\n",
      "doc_embedding_size 300\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "n_sents, n_words = 123, 40\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch = input_pipeline(\n",
    "            triples_train[:64*2], batch_size=64, num_epochs=1)\n",
    "        X = tf.reshape(\n",
    "            tf.transpose([anchor_batch, positive_batch, negative_batch],\n",
    "                         [1, 0, 2, 3]), [-1, n_sents, n_words],\n",
    "            name='X')\n",
    "\n",
    "        with tf.name_scope('init_model'):\n",
    "            model = TextCNN(\n",
    "                n_sents,\n",
    "                n_words,\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                sent_filter_sizes=[3, 4, 5],\n",
    "                sent_nb_filter=50,\n",
    "                doc_filter_sizes=[1, 2, 3],\n",
    "                doc_nb_filter=10,\n",
    "                sent_kmax=4,\n",
    "                doc_kmax=10,\n",
    "                learning_rate=0.001)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        print('sent_embedding_size %s' % model.sent_embedding_size.eval())\n",
    "        print('doc_embedding_size %s' % model.doc_embedding_size.eval())\n",
    "\n",
    "        saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "        # ===================summary====================\n",
    "        tf.summary.scalar(\"loss\", model.loss_op)\n",
    "        tf.summary.histogram(\"anchor\", model.anchor)\n",
    "        tf.summary.histogram(\"positive\", model.positive)\n",
    "        tf.summary.histogram(\"negative\", model.negative)\n",
    "        # Create summaries to visualize weights\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name.replace(':', '_'), var)\n",
    "        # Summarize all gradients\n",
    "        for grad, var in model.gradients:\n",
    "            tf.summary.histogram(\n",
    "                var.name.replace(':', '_') + '/gradient', grad)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            join(DATA_FOLDER, 'summary', 'train',\n",
    "                 str(datetime.datetime.now())), sess.graph)\n",
    "        # ===================summary====================\n",
    "\n",
    "        # Assign word embeddings to variable W\n",
    "        #!!!! index is shifted by 1\n",
    "        sess.run(\n",
    "            model.embedding_init,\n",
    "            feed_dict={model.embedding_placeholder: word_embeddings})\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss, summary = sess.run([\n",
    "                    train_op, model.global_step, model.loss_op,\n",
    "                    merged_summary_op\n",
    "                ])\n",
    "                current_step = tf.train.global_step(sess, model.global_step)\n",
    "                train_writer.add_summary(summary, current_step)\n",
    "                print(current_step, loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "    saver.save(\n",
    "        sess,\n",
    "        join(DATA_FOLDER, 'models', '%s' % str(datetime.datetime.now())),\n",
    "        global_step=current_step)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:52:27.346292Z",
     "start_time": "2017-08-24T19:52:27.027470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-24 22:52:24.976487-2.data-00000-of-00001\r\n",
      "2017-08-24 22:52:24.976487-2.index\r\n",
      "2017-08-24 22:52:24.976487-2.meta\r\n",
      "checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "!ls {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T20:06:15.688027Z",
     "start_time": "2017-08-24T20:05:51.072578Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../data/models/2017-08-24 22:52:24.976487-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 23:05:53,488 : INFO : Restoring parameters from ../data/models/2017-08-24 22:52:24.976487-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 300, 1)\n",
      "(192, 300, 1)\n",
      "Done testing -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "model_dir = join(DATA_FOLDER, 'models')\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "n_sents, n_words = 123, 40\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        #First let's load meta graph and restore weights\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            join(model_dir, '2017-08-24 22:52:24.976487-2.meta'))\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "\n",
    "        # Now, let's access and create placeholders variables and\n",
    "        graph = tf.get_default_graph()\n",
    "        doc_embed = graph.get_operation_by_name(\n",
    "            \"optimize/loss/doc_embed_normalized\").outputs[0]\n",
    "        X = graph.get_operation_by_name(\"X\").outputs[0]\n",
    "        \n",
    "        anchor_batch, positive_batch, negative_batch = input_pipeline(\n",
    "            triples_test[:64], batch_size=64, num_epochs=1, num_threads=1)\n",
    "        X = tf.reshape(\n",
    "            tf.transpose([anchor_batch, positive_batch, negative_batch],\n",
    "                         [1, 0, 2, 3]), [-1, n_sents, n_words],\n",
    "            name='X')        \n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "#         print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "                \n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                res = sess.run(doc_embed)\n",
    "                print(res.shape)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done testing -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T19:01:19.008039Z",
     "start_time": "2017-08-24T19:01:19.002576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 300, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T09:16:07.024602Z",
     "start_time": "2017-08-22T09:16:07.019031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.2293906810037"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200000*1000/(93*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:33:14.889183Z",
     "start_time": "2017-08-24T12:33:14.859352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[25 26]\n",
      "  [27 28]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]]\n",
      "\n",
      " [[29 30]\n",
      "  [31 32]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[21 22]\n",
      "  [23 24]]\n",
      "\n",
      " [[33 34]\n",
      "  [35 36]]]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "\n",
    "        a = tf.convert_to_tensor([[[1,2],[3,4]],[[5,6],[7,8]],[[9,10],[11,12]]])\n",
    "        b = tf.convert_to_tensor([[[13,14],[15,16]],[[17,18],[19,20]],[[21,22],[23,24]]])\n",
    "        c = tf.convert_to_tensor([[[25,26],[27,28]],[[29,30],[31,32]],[[33,34],[35,36]]])\n",
    "        \n",
    "        d = tf.reshape(tf.transpose([a,b,c], [1,0,2,3]), [-1,2,2])\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        [res] = sess.run([d])\n",
    "        print(res)\n",
    "\n",
    "#         train_writer = tf.summary.FileWriter('../data/summary', sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
