{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:21:17.538725Z",
     "start_time": "2017-08-23T10:21:14.620107Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "import tflearn.helpers.summarizer as s\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:09.806071Z",
     "start_time": "2017-08-23T10:20:03.352690Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-23 10:20:03,354 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-23 10:20:06,546 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-23 10:20:06,548 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-23 10:20:06,849 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-23 10:20:06,851 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-23 10:20:07,159 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-23 10:20:07,160 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:12.062139Z",
     "start_time": "2017-08-23T10:20:09.808583Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:16.821848Z",
     "start_time": "2017-08-23T10:20:12.064429Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.849548Z",
     "start_time": "2017-08-23T10:20:16.824501Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193705\n",
       "std           0.070574\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.857777Z",
     "start_time": "2017-08-23T10:20:54.851863Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18944918087888768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.249141Z",
     "start_time": "2017-08-23T10:20:54.859933Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.988410Z",
     "start_time": "2017-08-23T10:20:55.251655Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:21:21.965411Z",
     "start_time": "2017-08-23T10:21:18.836254Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:21:32.422200Z",
     "start_time": "2017-08-23T10:21:26.480409Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:21:35.480741Z",
     "start_time": "2017-08-23T10:21:35.466941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random select nagative examples\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def full_name(_id):\n",
    "    return join(DATA_FOLDER, 'corpus/%s.txt' % _id)\n",
    "\n",
    "def random_triples(sims, ids, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Get random triples, select negatives at random in each epoch.\n",
    "    Output: [anchor, positive, negative]\n",
    "    \"\"\"\n",
    "    ixs = list(range(len(ids)))\n",
    "    for ep in range(num_epochs):\n",
    "        random.shuffle(ixs)\n",
    "        it = iter(ixs)\n",
    "        for k, v in tqdm(sims.items()):\n",
    "            exclude = [full_name(i) for i in [k] + v]\n",
    "            for vi in v:\n",
    "                ix = next(it)\n",
    "                _neg = ids[ix]\n",
    "                while _neg in exclude:\n",
    "                    ix = next(it)\n",
    "                    _neg = ids[ix]\n",
    "                yield [full_name(k), full_name(vi), _neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:21:40.258267Z",
     "start_time": "2017-08-23T10:21:40.230982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    with tf.name_scope('parse_csv'):\n",
    "        strings = tf.string_split([text], delimiter='\\n')\n",
    "        raw_nums = tf.string_split(strings.values)\n",
    "        nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "        dense = tf.sparse_to_dense(\n",
    "            raw_nums.indices, raw_nums.dense_shape, nums, default_value=0)\n",
    "        dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    with tf.name_scope('read_input_tuple'):\n",
    "        fnames = filename_queue.dequeue()\n",
    "        example = []\n",
    "        for fn in tf.unstack(fnames):\n",
    "            record_string = tf.read_file(fn)\n",
    "            arr = parse_csv(record_string)\n",
    "            example.append(arr)\n",
    "    return example\n",
    "\n",
    "def input_pipeline(triples, batch_size, num_epochs=1):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        triples, num_epochs=num_epochs, capacity=32, shuffle=True, seed=0)\n",
    "    example = read_input_tuple(filename_queue)\n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    anchor, positive, negative = tf.train.batch(\n",
    "        example,\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        #         allow_smaller_final_batch=True,\n",
    "        num_threads=cpu_count)\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:35:34.055897Z",
     "start_time": "2017-08-23T10:35:33.834086Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 n_sents,\n",
    "                 n_words,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 sent_filter_sizes=[2,3,4,5],\n",
    "                 sent_nb_filter=15,\n",
    "                 doc_filter_sizes=[1,2,3],\n",
    "                 doc_nb_filter=10,\n",
    "                 sent_kmax=10,\n",
    "                 doc_kmax=10):\n",
    "        self.n_sents = n_sents\n",
    "        self.n_words = n_words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sent_filter_sizes = sent_filter_sizes\n",
    "        self.sent_nb_filter = sent_nb_filter\n",
    "        self.doc_filter_sizes = doc_filter_sizes\n",
    "        self.doc_nb_filter = doc_nb_filter\n",
    "        self.sent_kmax = sent_kmax\n",
    "        self.doc_kmax = doc_kmax\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.constant(0.0, shape=[self.vocab_size, self.embedding_size]),\n",
    "                trainable=False,\n",
    "                name=\"W\")\n",
    "\n",
    "            self.embedding_placeholder = tf.placeholder(\n",
    "                tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "\n",
    "            embedded_words = tf.nn.embedding_lookup(W, X)\n",
    "            self.embedded_words_expanded = tf.expand_dims(embedded_words, -1)\n",
    "\n",
    "        with tf.variable_scope('sent'):\n",
    "            self._create_sharable_weights(sent_filter_sizes, embedding_size,\n",
    "                                          sent_nb_filter)\n",
    "            self.sent_embedding_size = tf.convert_to_tensor(\n",
    "                sent_kmax * sent_nb_filter * len(sent_filter_sizes))\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            self._create_sharable_weights(\n",
    "                doc_filter_sizes, self.sent_embedding_size, doc_nb_filter)\n",
    "            self.doc_embedding_size = tf.convert_to_tensor(\n",
    "                doc_kmax * doc_nb_filter * len(doc_filter_sizes))\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\" This is the forward calculation from batch X to doc embeddins \"\"\"\n",
    "        with tf.variable_scope('sent'):\n",
    "            def convolv_on_sents(embeds):\n",
    "                return self._convolv_on_embeddings(\n",
    "                    embeds, self.sent_filter_sizes, self.sent_nb_filter,\n",
    "                    self.sent_kmax)\n",
    "            # iter over each document\n",
    "            sent_embed = tf.map_fn(\n",
    "                convolv_on_sents,\n",
    "                self.embedded_words_expanded,\n",
    "                parallel_iterations=10,\n",
    "                name='iter_over_docs')\n",
    "            # sent_embed shape is [batch, n_sent, sent_sent_kmax*sent_nb_filter*len(sent_filter_sizes), 1]\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            # finally, convolv on documents\n",
    "            doc_embed = self._convolv_on_embeddings(\n",
    "                sent_embed, self.doc_filter_sizes, self.doc_nb_filter,\n",
    "                self.doc_kmax)\n",
    "            # doc_embed shape is [batch, doc_kmax*doc_nb_filter*len(doc_filter_sizes), 1]\n",
    "\n",
    "        doc_embed_normalized = tf.nn.l2_normalize(\n",
    "            doc_embed, dim=1, name='L2_nomalization')\n",
    "\n",
    "        anchor, positive, negative = tf.unstack(\n",
    "            tf.reshape(doc_embed_normalized, [-1, 3, self.doc_embedding_size]),\n",
    "            3, 1)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def loss(self, X):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            anchor_embed, positive_embed, negative_embed = self.inference(X)\n",
    "            _loss = self.triplet_loss(anchor_embed, positive_embed,\n",
    "                                      negative_embed)\n",
    "        return _loss\n",
    "\n",
    "    def optimize(self, X):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            self.loss_op = self.loss(X)\n",
    "            self.gradients = self.optimizer.compute_gradients(self.loss_op)\n",
    "            apply_gradient_op = self.optimizer.apply_gradients(\n",
    "                self.gradients, global_step=self.global_step)\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def triplet_loss(self,\n",
    "                     anchor_embed,\n",
    "                     positive_embed,\n",
    "                     negative_embed,\n",
    "                     margin=0.2):\n",
    "        \"\"\"\n",
    "        input: Three L2 normalized tensors of shape [None, dim], compute on a batch\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('triplet_loss'):\n",
    "            d_pos = tf.reduce_sum(tf.square(anchor_embed - positive_embed), 1)\n",
    "            d_neg = tf.reduce_sum(tf.square(anchor_embed - negative_embed), 1)\n",
    "\n",
    "            loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _convolv_on_embeddings(self, embeds, filter_sizes, nb_filter, kmax):\n",
    "        \"\"\"\n",
    "        Create a convolution + k-max pool layer for each filter size, then concat and vectorize.\n",
    "        embeds shape is [batch, (n_words or n_sents), embedding_size, 1]\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for fsize in filter_sizes:\n",
    "            with tf.name_scope(\"conv-%s\" % fsize):\n",
    "                with tf.variable_scope(\n",
    "                        \"conv_weights_fsize-%s\" % fsize, reuse=True):\n",
    "                    weights_init = tf.get_variable('W')\n",
    "                    bias_init = tf.get_variable('b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embeds,\n",
    "                    weights_init,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, bias_init), name=\"relu\")\n",
    "#                 tf.summary.histogram(\"relu\", h)\n",
    "                # h shape is [batch, n_words - fsize + 1, 1, nb_filter]\n",
    "            with tf.name_scope('%s-maxpool-fsize-%s' % (kmax, fsize)):\n",
    "                # k-maxpooling over the outputs\n",
    "                trans = tf.transpose(h, perm=[0, 2, 3, 1])\n",
    "                values, indices = tf.nn.top_k(trans, k=kmax, sorted=False)\n",
    "                pooled = tf.transpose(values, perm=[0, 3, 1, 2])\n",
    "                # pooled shape is [batch, kmax, 1, nb_filter]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        with tf.name_scope('concat_and_vectorize'):\n",
    "            # Combine all the pooled features\n",
    "            h_pool = tf.concat(pooled_outputs, 3)\n",
    "            # h_pool shape is [batch, kmax, 1, nb_filter*len(filter_sizes)]\n",
    "\n",
    "            # Vectorize filters for each sent to get sent embeddings\n",
    "            trans = tf.transpose(h_pool, perm=[0, 2, 3, 1])\n",
    "            batch = tf.shape(embeds)[0]\n",
    "            sent_embed = tf.reshape(trans, [batch, -1, 1])\n",
    "            # sent_embed shape is [batch, kmax*nb_filter*len(filter_sizes), 1]\n",
    "\n",
    "        return sent_embed\n",
    "\n",
    "    def _create_sharable_weights(self, filter_sizes, embedding_size,\n",
    "                                 nb_filter):\n",
    "        \"\"\" Create sharable weights for each type of convolution \"\"\"\n",
    "        with tf.name_scope('sharable_weights'):\n",
    "            for fsize in filter_sizes:\n",
    "                with tf.variable_scope(\"conv_weights_fsize-%s\" % fsize):\n",
    "                    filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                    weights_init = tf.get_variable(\n",
    "                        'W',\n",
    "                        initializer=tf.truncated_normal(\n",
    "                            filter_shape, stddev=0.1))\n",
    "                    bias_init = tf.get_variable(\n",
    "                        'b', initializer=tf.constant(0.1, shape=[nb_filter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:52:55.628897Z",
     "start_time": "2017-08-23T10:52:54.716370Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 5062.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[u'../data/corpus/5984c70cb6b113634a638500.txt',\n",
       "  u'../data/corpus/5984bf3bb6b1130671638548.txt',\n",
       "  '../data/corpus/5984d34eb6b1135e1863852d.txt'],\n",
       " [u'../data/corpus/5984c70cb6b113634a638500.txt',\n",
       "  u'../data/corpus/5984beb2b6b113013c638524.txt',\n",
       "  '../data/corpus/5984d63fb6b1137992638545.txt'],\n",
       " [u'../data/corpus/5984c70cb6b113634a638500.txt',\n",
       "  u'../data/corpus/5984bdedb6b113758c63854a.txt',\n",
       "  '../data/corpus/5984dcf4b6b1133151638500.txt'],\n",
       " [u'../data/corpus/5984d6d1b6b1137def63852a.txt',\n",
       "  u'../data/corpus/5984c50bb6b1134d65638514.txt',\n",
       "  '../data/corpus/5984d8a1b6b11311bf63853a.txt'],\n",
       " [u'../data/corpus/5984bd59b6b11370e8638549.txt',\n",
       "  u'../data/corpus/5984b70db6b1131ad563854e.txt',\n",
       "  '../data/corpus/5984b87ab6b11332a3638520.txt']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1000 = random.sample(list(sims), 64*5)\n",
    "triples = list(random_triples({k:sims[k] for k in ids1000}, ids, num_epochs=1))\n",
    "triples[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:52:55.635216Z",
     "start_time": "2017-08-23T10:52:55.630975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:57:24.164726Z",
     "start_time": "2017-08-23T10:52:58.409731Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.18648443)\n",
      "(2, 0.19500262)\n",
      "(3, 0.20784768)\n",
      "(4, 0.19668686)\n",
      "(5, 0.19380738)\n",
      "(6, 0.20171696)\n",
      "(7, 0.2020243)\n",
      "(8, 0.20092303)\n",
      "(9, 0.20554084)\n",
      "(10, 0.19718865)\n",
      "Done training -- epoch limit reached\n",
      "--- 265.688844204 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "# X = tf.placeholder(tf.int32, [None, n_sents, n_words], name=\"X\")\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch = input_pipeline(\n",
    "            triples, batch_size=64, num_epochs=1)\n",
    "        X = tf.concat(\n",
    "            [anchor_batch, positive_batch, negative_batch],\n",
    "            axis=0,\n",
    "            name='concat_tupples')\n",
    "\n",
    "        with tf.name_scope('init_model'):\n",
    "            with tf.name_scope('batch_shape'):\n",
    "                n_sents = tf.shape(X)[1]\n",
    "                n_words = tf.shape(X)[2]\n",
    "            model = TextCNN(\n",
    "                n_sents,\n",
    "                n_words,\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                sent_filter_sizes=[2, 3, 4, 5],\n",
    "                sent_nb_filter=15,\n",
    "                doc_filter_sizes=[1, 2, 3],\n",
    "                doc_nb_filter=10,\n",
    "                sent_kmax=10,\n",
    "                doc_kmax=10)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        tf.summary.scalar(\"loss\", model.loss_op)\n",
    "        # Create summaries to visualize weights\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name.replace(':', '_'), var)\n",
    "        # Summarize all gradients\n",
    "        for grad, var in model.gradients:\n",
    "            tf.summary.histogram(var.name.replace(':', '_') + '/gradient', grad)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('../data/summary', sess.graph)\n",
    "\n",
    "        # Assign word embeddings to variable W\n",
    "        #!!!! index is shifted by 1\n",
    "        sess.run(\n",
    "            model.embedding_init,\n",
    "            feed_dict={model.embedding_placeholder: word_embeddings})\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss, summary = sess.run([\n",
    "                    train_op, model.global_step, model.loss_op,\n",
    "                    merged_summary_op\n",
    "                ])\n",
    "                current_step = tf.train.global_step(sess, model.global_step)\n",
    "                train_writer.add_summary(summary, current_step)\n",
    "                print(current_step, loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T09:16:07.024602Z",
     "start_time": "2017-08-22T09:16:07.019031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.2293906810037"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200000*1000/(93*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T08:04:57.247864Z",
     "start_time": "2017-08-22T08:04:57.181362Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 44]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "\n",
    "        with tf.variable_scope('var'):\n",
    "            t = tf.get_variable('w', initializer=tf.constant(0), dtype=tf.int32)\n",
    "        \n",
    "        r = t*2\n",
    "        \n",
    "        with tf.variable_scope('var', reuse=True):\n",
    "            tt = tf.get_variable('w', initializer=tf.constant(0), dtype=tf.int32)\n",
    "        \n",
    "        s = tt+44\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        res = sess.run([t, r, s])\n",
    "        print(res)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('../data/summary', sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
