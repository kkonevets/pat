{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:34.829275Z",
     "start_time": "2017-09-15T11:43:31.877797Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import os, time\n",
    "import tflearn\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "import cnn\n",
    "import data_flow\n",
    "\n",
    "from __future__ import division\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T16:14:46.853741Z",
     "start_time": "2017-09-08T16:14:42.297088Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-08 19:14:42,298 [MainThread  ] [INFO ]  loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-09-08 19:14:44,446 [MainThread  ] [INFO ]  loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-09-08 19:14:44,447 [MainThread  ] [INFO ]  loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-09-08 19:14:44,594 [MainThread  ] [INFO ]  setting ignored attribute syn0norm to None\n",
      "2017-09-08 19:14:44,595 [MainThread  ] [INFO ]  loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-09-08 19:14:44,736 [MainThread  ] [INFO ]  setting ignored attribute cum_table to None\n",
      "2017-09-08 19:14:44,737 [MainThread  ] [INFO ]  loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:55.126588Z",
     "start_time": "2017-09-08T10:58:53.880286Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T10:58:57.877549Z",
     "start_time": "2017-09-08T10:58:55.128389Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T12:29:29.157151Z",
     "start_time": "2017-09-06T12:28:38.836197Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193705\n",
       "std           0.070574\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T12:29:29.175545Z",
     "start_time": "2017-09-06T12:29:29.159459Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20150940312078128"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T12:29:29.621377Z",
     "start_time": "2017-09-06T12:29:29.177828Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T12:29:36.303189Z",
     "start_time": "2017-09-06T12:29:29.623782Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:38.163778Z",
     "start_time": "2017-09-15T11:43:34.831255Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:41.747257Z",
     "start_time": "2017-09-15T11:43:38.165485Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.222479Z",
     "start_time": "2017-09-15T11:43:41.749250Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368458/368458 [00:07<00:00, 51165.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(619669, 68853, 76503)\n"
     ]
    }
   ],
   "source": [
    "triples_all = list(data_flow.random_triples(sims, ids, num_epochs=1, seed=1))\n",
    "_triples, triples_test = train_test_split(triples_all, test_size=0.1, random_state=0)\n",
    "triples_train, triples_val = train_test_split(_triples, test_size=0.1, random_state=0)\n",
    "\n",
    "print(len(triples_train), len(triples_val), len(triples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.226494Z",
     "start_time": "2017-09-15T11:43:51.224338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# triples_all = pd.read_csv(join(DATA_FOLDER, 'mpk_flow.csv.gz'), sep=' ', header=None)\n",
    "# triples_all = triples_all[triples_all[2] != '59a969f1782b1b893a912539']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.230727Z",
     "start_time": "2017-09-15T11:43:51.228364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_fname(_id):\n",
    "    return join(DATA_FOLDER, 'corpus', _id + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.235214Z",
     "start_time": "2017-09-15T11:43:51.232508Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# triples_train, triples_val, triples_test = [], [], []\n",
    "# for epoch_data in np.array_split(triples_all, 3)[:1]:\n",
    "#     _triples, _test = train_test_split(epoch_data, test_size=0.1, random_state=0, shuffle=False)\n",
    "#     _train, _val = train_test_split(_triples, test_size=0.1, random_state=0, shuffle=False)\n",
    "\n",
    "#     triples_train += [[add_fname(a), add_fname(b), add_fname(c)] for a,b,c in _train.values]\n",
    "#     triples_val += [[add_fname(a), add_fname(b), add_fname(c)] for a,b,c in _val.values]\n",
    "#     triples_test += [[add_fname(a), add_fname(b), add_fname(c)] for a,b,c in _test.values]\n",
    "    \n",
    "# logging.info('train %s val %s test %s' % (len(triples_train), len(triples_val), len(triples_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.243040Z",
     "start_time": "2017-09-15T11:43:51.236788Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "n_sents, n_words = 123, 40\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "sent_filter_sizes = [1, 2, 3, 4, 5]\n",
    "sent_nb_filter = 10\n",
    "sent_embed_size = 128\n",
    "sent_kmax = 4\n",
    "\n",
    "doc_filter_sizes = [1, 2, 3, 4, 5]\n",
    "doc_nb_filter = 10\n",
    "doc_embed_size = 200\n",
    "doc_kmax = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T14:05:34.046098Z",
     "start_time": "2017-09-14T14:04:31.129748Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-14 17:04:31,165 [MainThread  ] [INFO ]  start training ...\n",
      "2017-09-14 17:04:31,537 [MainThread  ] [INFO ]  sent_out_size 200, doc_out_size 200\n",
      "2017-09-14 17:04:31,538 [MainThread  ] [INFO ]  sent_embed_size 128, doc_embed_size 200\n",
      "2017-09-14 17:04:54,435 [MainThread  ] [INFO ]  step 1, loss 0.493278\n",
      "2017-09-14 17:05:13,541 [MainThread  ] [INFO ]  step 2, loss 0.453561\n",
      "2017-09-14 17:05:31,858 [MainThread  ] [INFO ]  step 3, loss 0.391074\n",
      "2017-09-14 17:05:31,861 [MainThread  ] [INFO ]  Done training -- epoch limit reached\n",
      "2017-09-14 17:05:34,042 [MainThread  ] [INFO ]  --- 62.8675031662 seconds ---\n"
     ]
    }
   ],
   "source": [
    "reload(cnn)\n",
    "\n",
    "logging.info('start training ...')\n",
    "start_time = time.time()\n",
    "\n",
    "finished = False\n",
    "try:\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        X, fnames_batch = data_flow.input_pipeline(\n",
    "            triples_train[:batch_size*3], [n_sents, n_words], \n",
    "            batch_size=batch_size, num_epochs=num_epochs)\n",
    "\n",
    "        model = cnn.TextCNN(\n",
    "            n_sents,\n",
    "            n_words,\n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            batch_size,\n",
    "            sent_filter_sizes=sent_filter_sizes,\n",
    "            sent_nb_filter=sent_nb_filter,\n",
    "            sent_embed_size=sent_embed_size,\n",
    "            doc_filter_sizes=doc_filter_sizes,\n",
    "            doc_nb_filter=doc_nb_filter,\n",
    "            doc_embed_size=doc_embed_size,\n",
    "            sent_kmax=sent_kmax,\n",
    "            doc_kmax=doc_kmax,\n",
    "            learning_rate=learning_rate)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        model.init_summary()\n",
    "        #!!!! index is shifted by 1\n",
    "        model.init_lookup_table(word_embeddings)\n",
    "\n",
    "#         pprint([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss, summary = sess.run([\n",
    "                    train_op, model.global_step, model.loss_op,\n",
    "                    model.merged_summary_op\n",
    "                ], feed_dict = {model.phase: 1})\n",
    "                model.add_summary(summary, step)\n",
    "                logging.info('step %s, loss %s' % (step, loss))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            logging.info('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        model.save(step)\n",
    "        finished = True\n",
    "\n",
    "except Exception as e:\n",
    "    logging.exception(\"train error\")\n",
    "    send_email('notebook_url', subject='train error', body=e)\n",
    "finally:\n",
    "    if finished:\n",
    "#     send_email('notebook_url', subject='finished training')\n",
    "        pass\n",
    "        \n",
    "logging.info(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.523914Z",
     "start_time": "2017-09-15T11:43:51.245448Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/models/2017-09-12 06:48:30.232017-3000.meta\r\n",
      "../data/models/2017-09-13 03:45:02.521079-9682.meta\r\n",
      "../data/models/2017-09-13 15:57:25.781316-3.meta\r\n",
      "../data/models/2017-09-13 19:15:36.168251-3.meta\r\n",
      "../data/models/2017-09-14 17:05:31.877463-3.meta\r\n",
      "../data/models/2017-09-15 02:18:39.674838-9682.meta\r\n"
     ]
    }
   ],
   "source": [
    "model_dir = join(DATA_FOLDER, 'models')\n",
    "!ls {model_dir+'/*.meta'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:51.537750Z",
     "start_time": "2017-09-15T11:43:51.529763Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_model(model_dir):\n",
    "    fnames = glob(join(model_dir, '*.meta'))\n",
    "    return max(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:52.787801Z",
     "start_time": "2017-09-15T11:43:51.543031Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-15 14:43:52,785 [MainThread  ] [INFO ]  len(chunked) = 422394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/corpus/5984b84fb6b1132eeb638535.txt']\n"
     ]
    }
   ],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "chunked = list(chunks(ids, 3))\n",
    "print(chunked.pop())\n",
    "logging.info('len(chunked) = %s' % len(chunked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/save_restore_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:43:52.795160Z",
     "start_time": "2017-09-15T11:43:52.792665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_embeds, fnames = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T12:55:52.548506Z",
     "start_time": "2017-09-13T12:55:52.533873Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-3fb362e3db47>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-3fb362e3db47>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    res = evaluate(best, gold)from matplotlib import pyplot as plt\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "finished = False\n",
    "try:\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(0)\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)    \n",
    "    with sess.as_default():\n",
    "        X, fnames_batch = data_flow.input_pipeline(\n",
    "            chunked[:batch_size*100], [n_sents, n_words], batch_size=batch_size, \n",
    "            num_epochs=1, shuffle=False)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        # do not restore before global initialization, otherwise all weights are set to default !!!\n",
    "        saver = tf.train.import_meta_graph(last_model(model_dir), input_map={'X':X})\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        op_name = 'optimize/loss/doc_embed_normalized'\n",
    "        doc_embed_normalized = graph.get_operation_by_name(op_name).outputs[0]\n",
    "\n",
    "        anchor, positive, negative = tf.unstack(\n",
    "            tf.reshape(doc_embed_normalized, [-1, 3, doc_kmax * doc_nb_filter * len(doc_filter_sizes)]),\n",
    "            3, 1)\n",
    "        _loss = cnn.triplet_loss(anchor, positive, negative)\n",
    "\n",
    "#         pprint([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        step = 0\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                [batch_embeds, loss, _names] = sess.run([doc_embed_normalized, \n",
    "                                                         _loss, fnames_batch])\n",
    "                doc_embeds.append(batch_embeds)\n",
    "                fnames += list(_names)\n",
    "                logging.info('step %s, loss %s' % (step, loss))\n",
    "                step+=1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            logging.info('Done testing -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        finished = True\n",
    "        \n",
    "except Exception as e:\n",
    "    logging.exception(\"test error\")\n",
    "#     send_email('notebook_url', subject='test error', body=e)\n",
    "finally:\n",
    "    if finished:\n",
    "#         send_email('notebook_url', subject='finished testing')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T15:31:09.982339Z",
     "start_time": "2017-09-12T15:31:09.962783Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = np.concatenate(fnames)\n",
    "doc_embeds_reshaped = np.reshape(np.concatenate(doc_embeds), [-1, doc_embed_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:47:13.477987Z",
     "start_time": "2017-09-15T11:47:13.475546Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "margin = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:46:06.008416Z",
     "start_time": "2017-09-15T11:46:06.006032Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save('../data/doc_embeds_reshaped_1ep.npy', doc_embeds_reshaped)\n",
    "# np.save('../data/fnames_1ep.npy', fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:47:15.957151Z",
     "start_time": "2017-09-15T11:47:15.235225Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_embeds_reshaped = np.load('../data/saved/doc_embeds_reshaped_1ep_%s.npy' % margin)\n",
    "fnames = np.load('../data/saved/fnames_1ep_%s.npy' % margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:47:15.962095Z",
     "start_time": "2017-09-15T11:47:15.958942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267008"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:44:03.066422Z",
     "start_time": "2017-09-15T11:44:03.042441Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(keys):\n",
    "    names = pd.Series(range(len(fnames)), index = fnames)\n",
    "    test_vecs = []\n",
    "    for k in keys:\n",
    "        ix = names[add_fname(k)]\n",
    "        tvec = doc_embeds_reshaped[ix]\n",
    "        test_vecs.append(tvec)\n",
    "\n",
    "    dists = euclidean_distances(test_vecs, doc_embeds_reshaped)\n",
    "    sorted_ixs = np.argsort(dists, axis=1)\n",
    "    preds = {}\n",
    "    for k, _ixs in zip(keys, sorted_ixs):\n",
    "        preds[k] = [basename(n).split('.')[0] for n in fnames[_ixs[1:201]]]\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:46:07.976484Z",
     "start_time": "2017-09-15T11:46:07.973413Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_FOLDER, 'gold_mongo.json'), 'r') as f:\n",
    "    gold = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-15T11:47:19.048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = predict(gold.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-15T11:47:19.053Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = evaluate(preds, gold)\n",
    "\n",
    "ax = result['acc10'].hist()\n",
    "ax.set_xlabel(\"acc10\")\n",
    "plt.show()\n",
    "\n",
    "ax = result['acc20'].hist()\n",
    "ax.set_xlabel(\"acc20\")\n",
    "plt.show()\n",
    "\n",
    "ax = result['acc200'].hist()\n",
    "ax.set_xlabel(\"acc200\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-15T11:47:20.384Z"
    }
   },
   "outputs": [],
   "source": [
    "k = preds.keys()[0]\n",
    "print(k)\n",
    "preds[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-08T14:25:11.326800Z",
     "start_time": "2017-09-08T14:25:11.199178Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      " 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        \n",
    "        r = list(range(100))\n",
    "        ziped = zip(r[:-1], r[1:])\n",
    "        t = tf.convert_to_tensor(ziped)\n",
    "        \n",
    "        def f(el):\n",
    "            return el[0]\n",
    "        \n",
    "#         elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "#         squares = tf.map_fn(lambda x: x * x, elems)\n",
    "\n",
    "        \n",
    "        m = tf.map_fn(f, t)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        [res] = sess.run([m])\n",
    "        print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
