{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:42:17.541481Z",
     "start_time": "2017-08-22T14:41:37.558681Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:31:16.402327Z",
     "start_time": "2017-08-22T14:31:05.984457Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-22 17:31:05,986 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-22 17:31:09,212 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-22 17:31:09,214 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-22 17:31:11,186 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-22 17:31:11,189 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-22 17:31:13,711 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-22 17:31:13,712 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193707\n",
       "std           0.070564\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19733612657600905"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:42:18.042985Z",
     "start_time": "2017-08-22T14:42:17.549047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:42:25.592614Z",
     "start_time": "2017-08-22T14:42:18.045195Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T15:06:10.552525Z",
     "start_time": "2017-08-22T15:06:10.522212Z"
    }
   },
   "outputs": [],
   "source": [
    "# random select nagative examples\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def full_name(_id):\n",
    "    return join(DATA_FOLDER, 'corpus/%s.txt' % _id)\n",
    "\n",
    "def random_triples(sims, ids, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Get random triples, select negatives at random in each epoch.\n",
    "    Output: [anchor, positive, negative]\n",
    "    \"\"\"\n",
    "    ixs = list(range(len(ids)))\n",
    "    for ep in range(num_epochs):\n",
    "        random.shuffle(ixs)\n",
    "        it = iter(ixs)\n",
    "        for k, v in tqdm(sims.items()):\n",
    "            exclude = [full_name(i) for i in [k] + v]\n",
    "            for vi in v:\n",
    "                ix = next(it)\n",
    "                _neg = ids[ix]\n",
    "                while _neg in exclude:\n",
    "                    ix = next(it)\n",
    "                    _neg = ids[ix]\n",
    "                yield [full_name(k), full_name(vi), _neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:42:39.285717Z",
     "start_time": "2017-08-22T14:42:25.622734Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368458/368458 [00:11<00:00, 31632.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['../data/corpus/5984ba17b6b11349f563850c.txt',\n",
       "  '../data/corpus/5984b787b6b113256063850b.txt',\n",
       "  '../data/corpus/5984c93bb6b113798f638503.txt'],\n",
       " ['../data/corpus/5984ba17b6b11349f563850d.txt',\n",
       "  '../data/corpus/5984b5ccb6b113073663853e.txt',\n",
       "  '../data/corpus/5984bd13b6b1136b70638512.txt'],\n",
       " ['../data/corpus/5984ba17b6b11349f5638510.txt',\n",
       "  '../data/corpus/5984b653b6b11311c963852f.txt',\n",
       "  '../data/corpus/5984dcffb6b113314463852e.txt'],\n",
       " ['../data/corpus/5984ba1db6b11349ee63852c.txt',\n",
       "  '../data/corpus/5984b6efb6b11319d463852b.txt',\n",
       "  '../data/corpus/5984b63db6b1130e54638503.txt'],\n",
       " ['../data/corpus/5984ba1db6b11349e863851e.txt',\n",
       "  '../data/corpus/5984b791b6b113254d63854d.txt',\n",
       "  '../data/corpus/5984c4b4b6b11347a263850c.txt']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples = list(random_triples(sims, ids, num_epochs=1))\n",
    "triples[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:58:37.033277Z",
     "start_time": "2017-08-22T14:58:36.948191Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    with tf.name_scope('parse_csv'):\n",
    "        strings = tf.string_split([text], delimiter='\\n')\n",
    "        raw_nums = tf.string_split(strings.values)\n",
    "        nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "        dense = tf.sparse_to_dense(\n",
    "            raw_nums.indices, raw_nums.dense_shape, nums, default_value=0)\n",
    "        dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    with tf.name_scope('read_input_tuple'):\n",
    "        fnames = filename_queue.dequeue()\n",
    "        example = []\n",
    "        for fn in tf.unstack(fnames):\n",
    "            record_string = tf.read_file(fn)\n",
    "            arr = parse_csv(record_string)\n",
    "            example.append(arr)\n",
    "    return example\n",
    "\n",
    "def input_pipeline(triples, batch_size, num_epochs=1):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        triples, num_epochs=num_epochs, capacity=32, shuffle=True, seed=0)\n",
    "    example = read_input_tuple(filename_queue)\n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    anchor, positive, negative = tf.train.batch(\n",
    "        example,\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        #         allow_smaller_final_batch=True,\n",
    "        num_threads=cpu_count)\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T13:46:25.187571Z",
     "start_time": "2017-08-22T13:46:24.202935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 n_sents,\n",
    "                 n_words,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 sent_filter_sizes=[3, 4, 5],\n",
    "                 sent_nb_filter=5,\n",
    "                 doc_filter_sizes=[3],\n",
    "                 doc_nb_filter=5,\n",
    "                 sent_kmax=10,\n",
    "                 doc_kmax=10):\n",
    "        self.n_sents = n_sents\n",
    "        self.n_words = n_words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sent_filter_sizes = sent_filter_sizes\n",
    "        self.sent_nb_filter = sent_nb_filter\n",
    "        self.doc_filter_sizes = doc_filter_sizes\n",
    "        self.doc_nb_filter = doc_nb_filter\n",
    "        self.sent_kmax = sent_kmax\n",
    "        self.doc_kmax = doc_kmax\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.constant(0.0, shape=[self.vocab_size, self.embedding_size]),\n",
    "                trainable=False,\n",
    "                name=\"W\")\n",
    "\n",
    "            self.embedding_placeholder = tf.placeholder(\n",
    "                tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "\n",
    "            embedded_words = tf.nn.embedding_lookup(W, X)\n",
    "            self.embedded_words_expanded = tf.expand_dims(embedded_words, -1)\n",
    "\n",
    "        with tf.variable_scope('sent'):\n",
    "            self._create_sharable_weights(sent_filter_sizes, embedding_size,\n",
    "                                          sent_nb_filter)\n",
    "            self.sent_embedding_size = tf.convert_to_tensor(\n",
    "                sent_kmax * sent_nb_filter * len(sent_filter_sizes))\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            self._create_sharable_weights(\n",
    "                doc_filter_sizes, self.sent_embedding_size, doc_nb_filter)\n",
    "            self.doc_embedding_size = tf.convert_to_tensor(\n",
    "                doc_kmax * doc_nb_filter * len(doc_filter_sizes))\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\" This is the forward calculation from batch X to doc embeddins \"\"\"\n",
    "        with tf.variable_scope('sent'):\n",
    "            # iter over each document\n",
    "            def convolv_on_sents(embeds):\n",
    "                return self._convolv_on_embeddings(\n",
    "                    embeds, self.sent_filter_sizes, self.sent_nb_filter,\n",
    "                    self.sent_kmax)\n",
    "\n",
    "            sent_embed = tf.map_fn(\n",
    "                convolv_on_sents,\n",
    "                self.embedded_words_expanded,\n",
    "                parallel_iterations=10,\n",
    "                name='iter_over_docs')\n",
    "            # sent_embed shape is [batch, n_sent, sent_sent_kmax*sent_nb_filter*len(sent_filter_sizes), 1]\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            # finally, convolv on documents\n",
    "            doc_embed = self._convolv_on_embeddings(\n",
    "                sent_embed, self.doc_filter_sizes, self.doc_nb_filter,\n",
    "                self.doc_kmax)\n",
    "            # doc_embed shape is [batch, doc_kmax*doc_nb_filter*len(doc_filter_sizes), 1]\n",
    "\n",
    "        doc_embed_normalized = tf.nn.l2_normalize(\n",
    "            doc_embed, dim=1, name='L2_nomalization')\n",
    "\n",
    "        anchor, positive, negative = tf.unstack(\n",
    "            tf.reshape(doc_embed_normalized, [-1, 3, self.doc_embedding_size]),\n",
    "            3, 1)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def loss(self, X):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            anchor_embed, positive_embed, negative_embed = self.inference(X)\n",
    "            _loss = self.triplet_loss(anchor_embed, positive_embed,\n",
    "                                      negative_embed)\n",
    "        return _loss\n",
    "\n",
    "    def optimize(self, X):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            self.loss_op = self.loss(X)\n",
    "            gradients = self.optimizer.compute_gradients(self.loss_op)\n",
    "            apply_gradient_op = self.optimizer.apply_gradients(\n",
    "                gradients, global_step=self.global_step)\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def triplet_loss(self,\n",
    "                     anchor_embed,\n",
    "                     positive_embed,\n",
    "                     negative_embed,\n",
    "                     margin=0.2):\n",
    "        \"\"\"\n",
    "        input: Three L2 normalized tensors of shape [None, dim], compute on a batch\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('triplet_loss'):\n",
    "            d_pos = tf.reduce_sum(tf.square(anchor_embed - positive_embed), 1)\n",
    "            d_neg = tf.reduce_sum(tf.square(anchor_embed - negative_embed), 1)\n",
    "\n",
    "            loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _convolv_on_embeddings(self, embeds, filter_sizes, nb_filter, kmax):\n",
    "        \"\"\"\n",
    "        Create a convolution + k-max pool layer for each filter size, then concat and vectorize.\n",
    "        embeds shape is [batch, (n_words or n_sents), embedding_size, 1]\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for fsize in filter_sizes:\n",
    "            with tf.name_scope(\"conv-%s\" % fsize):\n",
    "                embedding_size = embeds.get_shape()[2]\n",
    "                filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                with tf.variable_scope(\n",
    "                        \"conv_weights_fsize-%s\" % fsize, reuse=True):\n",
    "                    weights_init = tf.get_variable('W')\n",
    "                    bias_init = tf.get_variable('b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embeds,\n",
    "                    weights_init,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, bias_init), name=\"relu\")\n",
    "                # h shape is [batch, n_words - fsize + 1, 1, nb_filter]\n",
    "            with tf.name_scope('%s-maxpool-fsize-%s' % (kmax, fsize)):\n",
    "                # k-maxpooling over the outputs\n",
    "                trans = tf.transpose(h, perm=[0, 2, 3, 1])\n",
    "                values, indices = tf.nn.top_k(trans, k=kmax, sorted=False)\n",
    "                pooled = tf.transpose(values, perm=[0, 3, 1, 2])\n",
    "                # pooled shape is [batch, kmax, 1, nb_filter]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        with tf.name_scope('concat_and_vectorize'):\n",
    "            # Combine all the pooled features\n",
    "            h_pool = tf.concat(pooled_outputs, 3)\n",
    "            # h_pool shape is [batch, kmax, 1, nb_filter*len(filter_sizes)]\n",
    "\n",
    "            # Vectorize filters for each sent to get sent embeddings\n",
    "            trans = tf.transpose(h_pool, perm=[0, 2, 3, 1])\n",
    "            batch = tf.shape(embeds)[0]\n",
    "            sent_embed = tf.reshape(trans, [batch, -1, 1])\n",
    "            # sent_embed shape is [batch, kmax*nb_filter*len(filter_sizes), 1]\n",
    "\n",
    "        return sent_embed\n",
    "\n",
    "    def _create_sharable_weights(self, filter_sizes, embedding_size,\n",
    "                                 nb_filter):\n",
    "        \"\"\"\n",
    "        Create sharable weights for each type of convolution\n",
    "        \"\"\"\n",
    "        with tf.name_scope('create_sharable_weights'):\n",
    "            for fsize in filter_sizes:\n",
    "                with tf.variable_scope(\"conv_weights_fsize-%s\" % fsize):\n",
    "                    filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                    weights_init = tf.get_variable(\n",
    "                        'W',\n",
    "                        initializer=tf.truncated_normal(\n",
    "                            filter_shape, stddev=0.1))\n",
    "                    bias_init = tf.get_variable(\n",
    "                        'b', initializer=tf.constant(0.1, shape=[nb_filter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:31:01.359559Z",
     "start_time": "2017-08-22T13:52:29.006971Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.204674\n",
      "2 0.189512\n",
      "3 0.209556\n",
      "4 0.202505\n",
      "5 0.199706\n",
      "6 0.200652\n",
      "7 0.200674\n",
      "8 0.203983\n",
      "9 0.198738\n",
      "10 0.198783\n",
      "11 0.198995\n",
      "12 0.198132\n",
      "13 0.202118\n",
      "14 0.19416\n",
      "15 0.198964\n",
      "16 0.203873\n",
      "17 0.202643\n",
      "18 0.195779\n",
      "19 0.198251\n",
      "20 0.207572\n",
      "21 0.198374\n",
      "22 0.196565\n",
      "23 0.196155\n",
      "24 0.202713\n",
      "25 0.201249\n",
      "26 0.202366\n",
      "27 0.196048\n",
      "28 0.193387\n",
      "29 0.204323\n",
      "30 0.20016\n",
      "31 0.195729\n",
      "32 0.200769\n",
      "33 0.207825\n",
      "34 0.196705\n",
      "35 0.200252\n",
      "36 0.195519\n",
      "37 0.188737\n",
      "38 0.208881\n",
      "39 0.207034\n",
      "40 0.198689\n",
      "41 0.204933\n",
      "42 0.205622\n",
      "43 0.199487\n",
      "44 0.197411\n",
      "45 0.198704\n",
      "46 0.195825\n",
      "47 0.200896\n",
      "48 0.200864\n",
      "49 0.197909\n",
      "50 0.194899\n",
      "51 0.199352\n",
      "52 0.200419\n",
      "53 0.199837\n",
      "54 0.201269\n",
      "55 0.202574\n",
      "56 0.206371\n",
      "57 0.203607\n",
      "58 0.200674\n",
      "59 0.202371\n",
      "60 0.199754\n",
      "61 0.200145\n",
      "62 0.202011\n",
      "63 0.198619\n",
      "64 0.198909\n",
      "65 0.202707\n",
      "66 0.201365\n",
      "67 0.198502\n",
      "68 0.202026\n",
      "69 0.200825\n",
      "70 0.201257\n",
      "71 0.200128\n",
      "72 0.200188\n",
      "73 0.201909\n",
      "74 0.199729\n",
      "75 0.200933\n",
      "76 0.199507\n",
      "77 0.199923\n",
      "78 0.200495\n",
      "79 0.19946\n",
      "80 0.20073\n",
      "81 0.200178\n",
      "82 0.200984\n",
      "83 0.200304\n",
      "84 0.200279\n",
      "85 0.199316\n",
      "86 0.199502\n",
      "87 0.199877\n",
      "88 0.199841\n",
      "89 0.198982\n",
      "90 0.200104\n",
      "91 0.199427\n",
      "92 0.200685\n",
      "93 0.199569\n",
      "94 0.199488\n",
      "95 0.200779\n",
      "96 0.200501\n",
      "97 0.199608\n",
      "98 0.198991\n",
      "99 0.200048\n",
      "100 0.199586\n",
      "101 0.200483\n",
      "102 0.199654\n",
      "103 0.19959\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-8558aed71cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 _, step, loss = sess.run(\n\u001b[0;32m---> 71\u001b[0;31m                     [train_op, model.global_step, model.loss_op])\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "\n",
    "# X = tf.placeholder(tf.int32, [None, n_sents, n_words], name=\"X\")\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch = input_pipeline(\n",
    "            triples, batch_size=64, num_epochs=1)\n",
    "        X = tf.concat(\n",
    "            [anchor_batch, positive_batch, negative_batch],\n",
    "            axis=0,\n",
    "            name='concat_tupples')\n",
    "\n",
    "        with tf.name_scope('init_model'):\n",
    "            with tf.name_scope('batch_shape'):\n",
    "                n_sents = tf.shape(X)[1]\n",
    "                n_words = tf.shape(X)[2]\n",
    "            model = TextCNN(\n",
    "                n_sents,\n",
    "                n_words,\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                sent_filter_sizes=[3, 4, 5],\n",
    "                sent_nb_filter=5,\n",
    "                doc_filter_sizes=[3],\n",
    "                doc_nb_filter=5,\n",
    "                sent_kmax=10,\n",
    "                doc_kmax=10)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        # Assign word embeddings to variable W\n",
    "        #!!!! index is shifted by 1\n",
    "        sess.run(\n",
    "            model.embedding_init,\n",
    "            feed_dict={model.embedding_placeholder: word_embeddings})\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        #         try:\n",
    "        #             train_op = model.optimize(X)\n",
    "        #             [res] = sess.run([train_op])\n",
    "        #             print([t.shape for t in res])\n",
    "        #             #             print([t.shape for t in _])\n",
    "        #             print(111111111111111111111111111111111111111111)\n",
    "        #         except tf.errors.OutOfRangeError:\n",
    "        #             print('Done training -- epoch limit reached')\n",
    "        #         finally:\n",
    "        #             # When done, ask the threads to stop.\n",
    "        #             coord.request_stop()\n",
    "\n",
    "        #         # Wait for threads to finish.\n",
    "        #         coord.join(threads)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss = sess.run(\n",
    "                    [train_op, model.global_step, model.loss_op])\n",
    "                current_step = tf.train.global_step(sess, model.global_step)\n",
    "                print(current_step, loss)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('../data/summary', sess.graph)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T09:22:11.801299Z",
     "start_time": "2017-08-22T09:22:11.795160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19643182"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T09:16:07.024602Z",
     "start_time": "2017-08-22T09:16:07.019031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.2293906810037"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200000*1000/(93*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T08:04:57.247864Z",
     "start_time": "2017-08-22T08:04:57.181362Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 44]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "\n",
    "        with tf.variable_scope('var'):\n",
    "            t = tf.get_variable('w', initializer=tf.constant(0), dtype=tf.int32)\n",
    "        \n",
    "        r = t*2\n",
    "        \n",
    "        with tf.variable_scope('var', reuse=True):\n",
    "            tt = tf.get_variable('w', initializer=tf.constant(0), dtype=tf.int32)\n",
    "        \n",
    "        s = tt+44\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        res = sess.run([t, r, s])\n",
    "        print(res)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('../data/summary', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", wv.syn0[1], \"Dimensionality of word embedding\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# train/dev split here\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "#  If you would like TensorFlow to automatically choose an existing and supported device to \n",
    "#  run the operations in case the specified one doesn't exist, you can set allow_soft_placement to True\n",
    "    \n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = abspath(join(curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = abspath(join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = join(checkpoint_dir, \"model\")\n",
    "        if not exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Assign word embeddings to variable W\n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: wv.syn0}) #!!!! index is shifted by 1\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.66666666666666"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
