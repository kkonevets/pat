{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:21:49.986249Z",
     "start_time": "2017-08-25T14:21:49.978220Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "import tflearn.helpers.summarizer as s\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:09.806071Z",
     "start_time": "2017-08-23T10:20:03.352690Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-23 10:20:03,354 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-23 10:20:06,546 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-23 10:20:06,548 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-23 10:20:06,849 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-23 10:20:06,851 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-23 10:20:07,159 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-23 10:20:07,160 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:12.062139Z",
     "start_time": "2017-08-23T10:20:09.808583Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:16.821848Z",
     "start_time": "2017-08-23T10:20:12.064429Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.849548Z",
     "start_time": "2017-08-23T10:20:16.824501Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193705\n",
       "std           0.070574\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.857777Z",
     "start_time": "2017-08-23T10:20:54.851863Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18944918087888768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.249141Z",
     "start_time": "2017-08-23T10:20:54.859933Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.988410Z",
     "start_time": "2017-08-23T10:20:55.251655Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:05.965535Z",
     "start_time": "2017-08-25T14:22:01.035591Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:11.140550Z",
     "start_time": "2017-08-25T14:22:05.967871Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:11.159633Z",
     "start_time": "2017-08-25T14:22:11.146431Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random select nagative examples\n",
    "import random\n",
    "\n",
    "def full_name(_id):\n",
    "    return join(DATA_FOLDER, 'corpus/%s.txt' % _id)\n",
    "\n",
    "def random_triples(sims, ids, num_epochs=1, seed=0):\n",
    "    \"\"\"\n",
    "    Get random triples, select negatives at random in each epoch.\n",
    "    Output: [anchor, positive, negative]\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    ixs = list(range(len(ids)))\n",
    "    for ep in range(num_epochs):\n",
    "        random.shuffle(ixs)\n",
    "        it = iter(ixs)\n",
    "        for k, v in tqdm(sims.items()):\n",
    "            exclude = [full_name(i) for i in [k] + v]\n",
    "            for vi in v:\n",
    "                ix = next(it)\n",
    "                _neg = ids[ix]\n",
    "                while _neg in exclude:\n",
    "                    ix = next(it)\n",
    "                    _neg = ids[ix]\n",
    "                yield [full_name(k), full_name(vi), _neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:11.218213Z",
     "start_time": "2017-08-25T14:22:11.162090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    with tf.name_scope('parse_csv'):\n",
    "        strings = tf.string_split([text], delimiter='\\n')\n",
    "        raw_nums = tf.string_split(strings.values)\n",
    "        nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "        dense = tf.sparse_to_dense(\n",
    "            raw_nums.indices, raw_nums.dense_shape, nums, default_value=0)\n",
    "        dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    with tf.name_scope('read_input_tuple'):\n",
    "        fnames = filename_queue.dequeue()\n",
    "        example = []\n",
    "        for fn in tf.unstack(fnames):\n",
    "            record_string = tf.read_file(fn)\n",
    "            arr = parse_csv(record_string)\n",
    "            example.append(arr)\n",
    "        example.append(fnames)\n",
    "    return example\n",
    "\n",
    "def input_pipeline(triples, batch_size, num_epochs=1, num_threads=cpu_count, shuffle=True):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        triples, num_epochs=num_epochs, capacity=32, shuffle=shuffle, seed=0)\n",
    "    example = read_input_tuple(filename_queue)\n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    anchor, positive, negative, fnames = tf.train.batch(\n",
    "        example,\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        #         allow_smaller_final_batch=True,\n",
    "        num_threads=num_threads)\n",
    "    return anchor, positive, negative, fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:50:18.848878Z",
     "start_time": "2017-08-25T14:50:18.622981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 n_sents,\n",
    "                 n_words,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 sent_filter_sizes=[2, 3, 4, 5],\n",
    "                 sent_nb_filter=15,\n",
    "                 doc_filter_sizes=[1, 2, 3],\n",
    "                 doc_nb_filter=10,\n",
    "                 sent_kmax=10,\n",
    "                 doc_kmax=10,\n",
    "                 learning_rate=0.001):\n",
    "        with tf.name_scope('init_model'):\n",
    "            self.n_sents = n_sents\n",
    "            self.n_words = n_words\n",
    "            self.vocab_size = vocab_size\n",
    "            self.embedding_size = embedding_size\n",
    "            self.sent_filter_sizes = sent_filter_sizes\n",
    "            self.sent_nb_filter = sent_nb_filter\n",
    "            self.doc_filter_sizes = doc_filter_sizes\n",
    "            self.doc_nb_filter = doc_nb_filter\n",
    "            self.sent_kmax = sent_kmax\n",
    "            self.doc_kmax = doc_kmax\n",
    "            self.learning_rate = learning_rate\n",
    "            self.sess = tf.get_default_session()\n",
    "            \n",
    "            assert (self.sess is not None and not self.sess._closed)\n",
    "            \n",
    "            self.global_step = tf.get_variable(\"global_step\", initializer=tf.constant(0), trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "            # Embedding layer\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.LT = tf.get_variable('LT',\n",
    "                    initializer=tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                    trainable=False)\n",
    "\n",
    "                self.embedding_placeholder = tf.placeholder(\n",
    "                    tf.float32, [self.vocab_size, self.embedding_size])\n",
    "                self.embedding_init = self.LT.assign(self.embedding_placeholder)\n",
    "\n",
    "            with tf.variable_scope('sent'):\n",
    "                self._create_sharable_weights(sent_filter_sizes, embedding_size,\n",
    "                                              sent_nb_filter)\n",
    "                self.sent_embedding_size = tf.convert_to_tensor(\n",
    "                    sent_kmax * sent_nb_filter * len(sent_filter_sizes))\n",
    "\n",
    "            with tf.variable_scope('doc'):\n",
    "                self._create_sharable_weights(doc_filter_sizes,\n",
    "                                              self.sent_embedding_size.eval(),\n",
    "                                              doc_nb_filter)\n",
    "                self.doc_embedding_size = tf.convert_to_tensor(\n",
    "                    doc_kmax * doc_nb_filter * len(doc_filter_sizes))\n",
    "\n",
    "        print('sent_embedding_size %s' % self.sent_embedding_size.eval())\n",
    "        print('doc_embedding_size %s' % self.doc_embedding_size.eval())\n",
    "\n",
    "            \n",
    "    def inference(self, X):\n",
    "        \"\"\" This is the forward calculation from batch X to doc embeddins \"\"\"\n",
    "        \n",
    "        embedded_words = tf.nn.embedding_lookup(self.LT, X)\n",
    "        embedded_words_expanded = tf.expand_dims(embedded_words, -1)\n",
    "        \n",
    "        with tf.variable_scope('sent'):\n",
    "\n",
    "            def convolv_on_sents(embeds):\n",
    "                return self._convolv_on_embeddings(\n",
    "                    embeds, self.sent_filter_sizes, self.sent_nb_filter,\n",
    "                    self.sent_kmax)\n",
    "\n",
    "            # iter over each document\n",
    "            self.sent_embed = tf.map_fn(\n",
    "                convolv_on_sents,\n",
    "                embedded_words_expanded,\n",
    "                parallel_iterations=10,\n",
    "                name='iter_over_docs')\n",
    "            # sent_embed shape is [batch, n_sent, sent_sent_kmax*sent_nb_filter*len(sent_filter_sizes), 1]\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            # finally, convolv on documents\n",
    "            self.doc_embed = self._convolv_on_embeddings(\n",
    "                self.sent_embed, self.doc_filter_sizes, self.doc_nb_filter,\n",
    "                self.doc_kmax)\n",
    "            # doc_embed shape is [batch, doc_kmax*doc_nb_filter*len(doc_filter_sizes), 1]\n",
    "\n",
    "        doc_embed_normalized = tf.nn.l2_normalize(\n",
    "            self.doc_embed, dim=1, name='doc_embed_normalized')\n",
    "\n",
    "        return doc_embed_normalized\n",
    "\n",
    "    def loss(self, X):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            doc_embed_normalized = self.inference(X)\n",
    "            self.anchor, self.positive, self.negative = tf.unstack(\n",
    "                tf.reshape(doc_embed_normalized, [-1, 3, self.doc_embedding_size]),\n",
    "                3, 1)\n",
    "            _loss = triplet_loss(self.anchor, self.positive, self.negative)\n",
    "        return _loss\n",
    "\n",
    "    def optimize(self, X):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            self.loss_op = self.loss(X)\n",
    "            self.gradients = self.optimizer.compute_gradients(self.loss_op)\n",
    "            apply_gradient_op = self.optimizer.apply_gradients(\n",
    "                self.gradients, global_step=self.global_step)\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def _convolv_on_embeddings(self, embeds, filter_sizes, nb_filter, kmax):\n",
    "        \"\"\"\n",
    "        Create a convolution + k-max pool layer for each filter size, then concat and vectorize.\n",
    "        embeds shape is [batch, (n_words or n_sents), embedding_size, 1]\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for fsize in filter_sizes:\n",
    "            with tf.name_scope(\"conv-%s\" % fsize):\n",
    "                with tf.variable_scope(\n",
    "                        \"conv_weights_fsize-%s\" % fsize, reuse=True):\n",
    "                    weights_init = tf.get_variable('W')\n",
    "                    bias_init = tf.get_variable('b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embeds,\n",
    "                    weights_init,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, bias_init), name=\"relu\")\n",
    "#                 h shape is [batch, n_words - fsize + 1, 1, nb_filter]\n",
    "#             tf.summary.histogram(\"relu\", h)\n",
    "\n",
    "            with tf.name_scope('%s-maxpool-fsize-%s' % (kmax, fsize)):\n",
    "                # k-maxpooling over the outputs\n",
    "                trans = tf.transpose(h, perm=[0, 2, 3, 1])\n",
    "                values, indices = tf.nn.top_k(trans, k=kmax, sorted=False)\n",
    "                pooled = tf.transpose(values, perm=[0, 3, 1, 2])\n",
    "                # pooled shape is [batch, kmax, 1, nb_filter]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        with tf.name_scope('concat_and_vectorize'):\n",
    "            # Combine all the pooled features\n",
    "            h_pool = tf.concat(pooled_outputs, 3)\n",
    "            # h_pool shape is [batch, kmax, 1, nb_filter*len(filter_sizes)]\n",
    "\n",
    "            # Vectorize filters for each sent to get sent embeddings\n",
    "            trans = tf.transpose(h_pool, perm=[0, 2, 3, 1])\n",
    "            batch = tf.shape(embeds)[0]\n",
    "            sent_embed = tf.reshape(trans, [batch, -1, 1])\n",
    "            # sent_embed shape is [batch, kmax*nb_filter*len(filter_sizes), 1]\n",
    "\n",
    "        return sent_embed\n",
    "\n",
    "    def _create_sharable_weights(self, filter_sizes, embedding_size,\n",
    "                                 nb_filter):\n",
    "        \"\"\" Create sharable weights for each type of convolution \"\"\"\n",
    "        with tf.name_scope('sharable_weights'):\n",
    "            for fsize in filter_sizes:\n",
    "                with tf.variable_scope(\"conv_weights_fsize-%s\" % fsize):\n",
    "                    filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                    initializer = tf.contrib.layers.xavier_initializer_conv2d(\n",
    "                        uniform=True)\n",
    "#                     initializer=tf.truncated_normal(stddev=0.1))\n",
    "                    weights_init = tf.get_variable(\n",
    "                        'W', filter_shape, initializer=initializer)\n",
    "                    bias_init = tf.get_variable(\n",
    "                        'b', initializer=tf.constant(0.1, shape=[nb_filter]))\n",
    "    \n",
    "    def init_summary(self):\n",
    "        1\n",
    "        \n",
    "        \n",
    "def triplet_loss(anchor_embed,\n",
    "                 positive_embed,\n",
    "                 negative_embed,\n",
    "                 margin=0.2):\n",
    "    \"\"\"\n",
    "    input: Three L2 normalized tensors of shape [None, dim], compute on a batch\n",
    "    output: float\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('triplet_loss'):\n",
    "        d_pos = tf.reduce_sum(tf.square(anchor_embed - positive_embed), 1)\n",
    "        d_neg = tf.reduce_sum(tf.square(anchor_embed - negative_embed), 1)\n",
    "\n",
    "        loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:23.521306Z",
     "start_time": "2017-08-25T14:22:11.584053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368458/368458 [00:09<00:00, 40595.78it/s]\n"
     ]
    }
   ],
   "source": [
    "triples_all = list(random_triples(sims, ids, num_epochs=1, seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:25.170778Z",
     "start_time": "2017-08-25T14:22:23.523842Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_triples, triples_test = train_test_split(triples_all, test_size=0.2, random_state=0)\n",
    "triples_train, triples_val = train_test_split(_triples, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:22:25.179266Z",
     "start_time": "2017-08-25T14:22:25.173935Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428414, 183606, 153005)\n"
     ]
    }
   ],
   "source": [
    "print(len(triples_train), len(triples_val), len(triples_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:50:18.620375Z",
     "start_time": "2017-08-25T14:48:04.116235Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_embedding_size 600\n",
      "doc_embedding_size 300\n",
      "(1, 0.19535846)\n",
      "(2, 0.12479226)\n",
      "(3, 0.1467894)\n",
      "(4, 0.16934134)\n",
      "Done training -- epoch limit reached\n",
      "--- 134.442246914 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "n_sents, n_words = 123, 40\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch, fnames = input_pipeline(\n",
    "            triples_train[:64*4], batch_size=64, num_epochs=1)\n",
    "        X = tf.reshape(\n",
    "            tf.transpose([anchor_batch, positive_batch, negative_batch],\n",
    "                         [1, 0, 2, 3]), [-1, n_sents, n_words],\n",
    "            name='X')\n",
    "\n",
    "        model = TextCNN(\n",
    "            n_sents,\n",
    "            n_words,\n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            sent_filter_sizes=[3, 4, 5],\n",
    "            sent_nb_filter=50,\n",
    "            doc_filter_sizes=[1, 2, 3],\n",
    "            doc_nb_filter=10,\n",
    "            sent_kmax=4,\n",
    "            doc_kmax=10,\n",
    "            learning_rate=0.001)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # ===================summary====================\n",
    "        # Create summaries to visualize weights\n",
    "        tf.summary.scalar(\"loss\", model.loss_op)\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name.replace(':', '_'), var)\n",
    "        # Summarize all gradients\n",
    "        for grad, var in model.gradients:\n",
    "            tf.summary.histogram(\n",
    "                var.name.replace(':', '_') + '/gradient', grad)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            join(DATA_FOLDER, 'summary', 'train',\n",
    "                 str(datetime.datetime.now())), sess.graph)\n",
    "        # ===================summary====================\n",
    "\n",
    "        # Assign word embeddings to variable W\n",
    "        #!!!! index is shifted by 1\n",
    "        sess.run(\n",
    "            model.embedding_init,\n",
    "            feed_dict={model.embedding_placeholder: word_embeddings})\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss, summary, anchor = sess.run([\n",
    "                    train_op, model.global_step, model.loss_op,\n",
    "                    merged_summary_op, model.anchor\n",
    "                ])\n",
    "                current_step = tf.train.global_step(sess, model.global_step)\n",
    "                train_writer.add_summary(summary, current_step)\n",
    "                print(current_step, loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "    saver.save(\n",
    "        sess,\n",
    "        join(DATA_FOLDER, 'models', '%s' % str(datetime.datetime.now())),\n",
    "        global_step=current_step)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T08:36:14.069337Z",
     "start_time": "2017-08-25T08:36:13.749791Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-25 11:36:11.921470-100.data-00000-of-00001\r\n",
      "2017-08-25 11:36:11.921470-100.index\r\n",
      "2017-08-25 11:36:11.921470-100.meta\r\n",
      "checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "model_dir = join(DATA_FOLDER, 'models')\n",
    "!ls {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/4_Utils/save_restore_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T11:19:02.678447Z",
     "start_time": "2017-08-25T11:15:35.937812Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../data/models/2017-08-25 11:36:11.921470-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-25 14:15:39,698 : INFO : Restoring parameters from ../data/models/2017-08-25 11:36:11.921470-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((192, 300, 1), 0.03564262)\n",
      "((192, 300, 1), 0.015106287)\n",
      "((192, 300, 1), 0.01525973)\n",
      "((192, 300, 1), 0.027117617)\n",
      "((192, 300, 1), 0.020871785)\n",
      "((192, 300, 1), 0.020573003)\n",
      "((192, 300, 1), 0.02336137)\n",
      "((192, 300, 1), 0.023403049)\n",
      "((192, 300, 1), 0.013923426)\n",
      "((192, 300, 1), 0.030562393)\n",
      "((192, 300, 1), 0.035025455)\n",
      "((192, 300, 1), 0.023427295)\n",
      "((192, 300, 1), 0.025457731)\n",
      "((192, 300, 1), 0.0093168225)\n",
      "((192, 300, 1), 0.020863641)\n",
      "((192, 300, 1), 0.033610787)\n",
      "((192, 300, 1), 0.018748417)\n",
      "((192, 300, 1), 0.038897656)\n",
      "((192, 300, 1), 0.02230488)\n",
      "((192, 300, 1), 0.022732088)\n",
      "Done testing -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "n_sents, n_words = 123, 40\n",
    "\n",
    "doc_filter_sizes=[1, 2, 3]\n",
    "doc_nb_filter=10\n",
    "doc_kmax=10\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch, fnames_batch = input_pipeline(\n",
    "            triples_test[:64*20], batch_size=64, num_epochs=1, num_threads=1, shuffle=False)\n",
    "        X = tf.reshape(\n",
    "            tf.transpose([anchor_batch, positive_batch, negative_batch],\n",
    "                         [1, 0, 2, 3]), [-1, n_sents, n_words],\n",
    "            name='X')\n",
    "        \n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "        # do not restore before global initialization, otherwise all weights are set to default !!!\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            join(model_dir, '2017-08-25 11:36:11.921470-100.meta'), input_map={'X':X})\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_dir))\n",
    "        graph = tf.get_default_graph()\n",
    "    \n",
    "        doc_embed_normalized = graph.get_operation_by_name('optimize/loss/doc_embed_normalized').outputs[0]\n",
    "        \n",
    "        anchor, positive, negative = tf.unstack(\n",
    "            tf.reshape(doc_embed_normalized, [-1, 3, doc_kmax * doc_nb_filter * len(doc_filter_sizes)]),\n",
    "            3, 1)\n",
    "        _loss = triplet_loss(anchor, positive, negative)\n",
    "                             \n",
    "#         pprint([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "        doc_embeds, fnames = [], []\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                [batch_embeds, loss, _names] = sess.run([doc_embed_normalized, _loss, fnames_batch])\n",
    "                doc_embeds.append(batch_embeds)\n",
    "                fnames += list(_names)\n",
    "                print(batch_embeds.shape, loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done testing -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T11:19:02.687926Z",
     "start_time": "2017-08-25T11:19:02.681539Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnames = np.concatenate(fnames)\n",
    "doc_embeds = np.reshape(np.concatenate(doc_embeds), [-1, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T11:44:38.977490Z",
     "start_time": "2017-08-25T11:44:38.970723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T12:07:01.170793Z",
     "start_time": "2017-08-25T12:07:01.167267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fn = fnames[6]\n",
    "test_vec = doc_embeds[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T12:07:03.811108Z",
     "start_time": "2017-08-25T12:07:03.805257Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists = np.sum(np.square(doc_embeds - test_vec), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T12:10:06.222567Z",
     "start_time": "2017-08-25T12:10:06.217138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/corpus/5984d324b6b1135cdf63851f.txt',\n",
       "       '../data/corpus/5984d599b6b11375b46384f9.txt',\n",
       "       '../data/corpus/5984d11ab6b113430363851e.txt',\n",
       "       '../data/corpus/5984c946b6b11379a663852a.txt',\n",
       "       '../data/corpus/5984d59fb6b11375c5638525.txt',\n",
       "       '../data/corpus/5984d016b6b1133c0063852f.txt',\n",
       "       '../data/corpus/5984c3b9b6b1133ef3638548.txt',\n",
       "       '../data/corpus/5984d2f2b6b1135aa963854a.txt',\n",
       "       '../data/corpus/5984dad2b6b1131d4b638520.txt',\n",
       "       '../data/corpus/5984c93ab6b11379ac638509.txt',\n",
       "       '../data/corpus/5984c968b6b1137bcd6384ee.txt',\n",
       "       '../data/corpus/5984c54fb6b113508c6384fa.txt',\n",
       "       '../data/corpus/5984ca5bb6b11304b8638501.txt',\n",
       "       '../data/corpus/5984d32db6b1135cda638533.txt',\n",
       "       '../data/corpus/5984d2e9b6b1135aaa6384fd.txt'], dtype=object)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ixs = dists.argsort()[:15]\n",
    "fnames[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T11:28:30.779616Z",
     "start_time": "2017-08-25T11:28:30.772511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../data/corpus/5984c59cb6b11352d3638549.txt',\n",
       "       '../data/corpus/5984c78bb6b11367c1638521.txt',\n",
       "       '../data/corpus/5984c91fb6b1137864638533.txt',\n",
       "       '../data/corpus/5984c93eb6b113798f638507.txt',\n",
       "       '../data/corpus/5984c787b6b11367c6638505.txt'], dtype=object)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ixs = dists.argsort()[-5:][::-1]\n",
    "fnames[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T12:07:12.719135Z",
     "start_time": "2017-08-25T12:07:12.712024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/corpus/5984d324b6b1135cdf63851f.txt'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:33:14.889183Z",
     "start_time": "2017-08-24T12:33:14.859352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[25 26]\n",
      "  [27 28]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]]\n",
      "\n",
      " [[29 30]\n",
      "  [31 32]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[21 22]\n",
      "  [23 24]]\n",
      "\n",
      " [[33 34]\n",
      "  [35 36]]]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "\n",
    "        a = tf.convert_to_tensor([[[1,2],[3,4]],[[5,6],[7,8]],[[9,10],[11,12]]])\n",
    "        b = tf.convert_to_tensor([[[13,14],[15,16]],[[17,18],[19,20]],[[21,22],[23,24]]])\n",
    "        c = tf.convert_to_tensor([[[25,26],[27,28]],[[29,30],[31,32]],[[33,34],[35,36]]])\n",
    "        \n",
    "        d = tf.reshape(tf.transpose([a,b,c], [1,0,2,3]), [-1,2,2])\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        [res] = sess.run([d])\n",
    "        print(res)\n",
    "\n",
    "#         train_writer = tf.summary.FileWriter('../data/summary', sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
