{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T16:45:25.266998Z",
     "start_time": "2017-08-22T16:45:25.254887Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "import tflearn.helpers.summarizer as s\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:31:16.402327Z",
     "start_time": "2017-08-22T14:31:05.984457Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-22 17:31:05,986 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-22 17:31:09,212 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-22 17:31:09,214 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-22 17:31:11,186 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-22 17:31:11,189 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-22 17:31:13,711 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-22 17:31:13,712 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193707\n",
       "std           0.070564\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19733612657600905"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:42:18.042985Z",
     "start_time": "2017-08-22T14:42:17.549047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:42:25.592614Z",
     "start_time": "2017-08-22T14:42:18.045195Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T15:06:10.552525Z",
     "start_time": "2017-08-22T15:06:10.522212Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random select nagative examples\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def full_name(_id):\n",
    "    return join(DATA_FOLDER, 'corpus/%s.txt' % _id)\n",
    "\n",
    "def random_triples(sims, ids, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Get random triples, select negatives at random in each epoch.\n",
    "    Output: [anchor, positive, negative]\n",
    "    \"\"\"\n",
    "    ixs = list(range(len(ids)))\n",
    "    for ep in range(num_epochs):\n",
    "        random.shuffle(ixs)\n",
    "        it = iter(ixs)\n",
    "        for k, v in tqdm(sims.items()):\n",
    "            exclude = [full_name(i) for i in [k] + v]\n",
    "            for vi in v:\n",
    "                ix = next(it)\n",
    "                _neg = ids[ix]\n",
    "                while _neg in exclude:\n",
    "                    ix = next(it)\n",
    "                    _neg = ids[ix]\n",
    "                yield [full_name(k), full_name(vi), _neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:58:37.033277Z",
     "start_time": "2017-08-22T14:58:36.948191Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    with tf.name_scope('parse_csv'):\n",
    "        strings = tf.string_split([text], delimiter='\\n')\n",
    "        raw_nums = tf.string_split(strings.values)\n",
    "        nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "        dense = tf.sparse_to_dense(\n",
    "            raw_nums.indices, raw_nums.dense_shape, nums, default_value=0)\n",
    "        dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    with tf.name_scope('read_input_tuple'):\n",
    "        fnames = filename_queue.dequeue()\n",
    "        example = []\n",
    "        for fn in tf.unstack(fnames):\n",
    "            record_string = tf.read_file(fn)\n",
    "            arr = parse_csv(record_string)\n",
    "            example.append(arr)\n",
    "    return example\n",
    "\n",
    "def input_pipeline(triples, batch_size, num_epochs=1):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        triples, num_epochs=num_epochs, capacity=32, shuffle=True, seed=0)\n",
    "    example = read_input_tuple(filename_queue)\n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    anchor, positive, negative = tf.train.batch(\n",
    "        example,\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        #         allow_smaller_final_batch=True,\n",
    "        num_threads=cpu_count)\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T07:45:13.895120Z",
     "start_time": "2017-08-23T07:45:12.995740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 n_sents,\n",
    "                 n_words,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 sent_filter_sizes=[2,3,4,5],\n",
    "                 sent_nb_filter=15,\n",
    "                 doc_filter_sizes=[1,2,3],\n",
    "                 doc_nb_filter=10,\n",
    "                 sent_kmax=10,\n",
    "                 doc_kmax=10):\n",
    "        self.n_sents = n_sents\n",
    "        self.n_words = n_words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sent_filter_sizes = sent_filter_sizes\n",
    "        self.sent_nb_filter = sent_nb_filter\n",
    "        self.doc_filter_sizes = doc_filter_sizes\n",
    "        self.doc_nb_filter = doc_nb_filter\n",
    "        self.sent_kmax = sent_kmax\n",
    "        self.doc_kmax = doc_kmax\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.constant(0.0, shape=[self.vocab_size, self.embedding_size]),\n",
    "                trainable=False,\n",
    "                name=\"W\")\n",
    "\n",
    "            self.embedding_placeholder = tf.placeholder(\n",
    "                tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "\n",
    "            embedded_words = tf.nn.embedding_lookup(W, X)\n",
    "            self.embedded_words_expanded = tf.expand_dims(embedded_words, -1)\n",
    "\n",
    "        with tf.variable_scope('sent'):\n",
    "            self._create_sharable_weights(sent_filter_sizes, embedding_size,\n",
    "                                          sent_nb_filter)\n",
    "            self.sent_embedding_size = tf.convert_to_tensor(\n",
    "                sent_kmax * sent_nb_filter * len(sent_filter_sizes))\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            self._create_sharable_weights(\n",
    "                doc_filter_sizes, self.sent_embedding_size, doc_nb_filter)\n",
    "            self.doc_embedding_size = tf.convert_to_tensor(\n",
    "                doc_kmax * doc_nb_filter * len(doc_filter_sizes))\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\" This is the forward calculation from batch X to doc embeddins \"\"\"\n",
    "        with tf.variable_scope('sent'):\n",
    "            def convolv_on_sents(embeds):\n",
    "                return self._convolv_on_embeddings(\n",
    "                    embeds, self.sent_filter_sizes, self.sent_nb_filter,\n",
    "                    self.sent_kmax)\n",
    "            # iter over each document\n",
    "            sent_embed = tf.map_fn(\n",
    "                convolv_on_sents,\n",
    "                self.embedded_words_expanded,\n",
    "                parallel_iterations=10,\n",
    "                name='iter_over_docs')\n",
    "            # sent_embed shape is [batch, n_sent, sent_sent_kmax*sent_nb_filter*len(sent_filter_sizes), 1]\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            # finally, convolv on documents\n",
    "            doc_embed = self._convolv_on_embeddings(\n",
    "                sent_embed, self.doc_filter_sizes, self.doc_nb_filter,\n",
    "                self.doc_kmax)\n",
    "            # doc_embed shape is [batch, doc_kmax*doc_nb_filter*len(doc_filter_sizes), 1]\n",
    "\n",
    "        doc_embed_normalized = tf.nn.l2_normalize(\n",
    "            doc_embed, dim=1, name='L2_nomalization')\n",
    "\n",
    "        anchor, positive, negative = tf.unstack(\n",
    "            tf.reshape(doc_embed_normalized, [-1, 3, self.doc_embedding_size]),\n",
    "            3, 1)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def loss(self, X):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            anchor_embed, positive_embed, negative_embed = self.inference(X)\n",
    "            _loss = self.triplet_loss(anchor_embed, positive_embed,\n",
    "                                      negative_embed)\n",
    "        return _loss\n",
    "\n",
    "    def optimize(self, X):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            self.loss_op = self.loss(X)\n",
    "            self.gradients = self.optimizer.compute_gradients(self.loss_op)\n",
    "            apply_gradient_op = self.optimizer.apply_gradients(\n",
    "                self.gradients, global_step=self.global_step)\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def triplet_loss(self,\n",
    "                     anchor_embed,\n",
    "                     positive_embed,\n",
    "                     negative_embed,\n",
    "                     margin=0.2):\n",
    "        \"\"\"\n",
    "        input: Three L2 normalized tensors of shape [None, dim], compute on a batch\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('triplet_loss'):\n",
    "            d_pos = tf.reduce_sum(tf.square(anchor_embed - positive_embed), 1)\n",
    "            d_neg = tf.reduce_sum(tf.square(anchor_embed - negative_embed), 1)\n",
    "\n",
    "            loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _convolv_on_embeddings(self, embeds, filter_sizes, nb_filter, kmax):\n",
    "        \"\"\"\n",
    "        Create a convolution + k-max pool layer for each filter size, then concat and vectorize.\n",
    "        embeds shape is [batch, (n_words or n_sents), embedding_size, 1]\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for fsize in filter_sizes:\n",
    "            with tf.name_scope(\"conv-%s\" % fsize):\n",
    "                with tf.variable_scope(\n",
    "                        \"conv_weights_fsize-%s\" % fsize, reuse=True):\n",
    "                    weights_init = tf.get_variable('W')\n",
    "                    bias_init = tf.get_variable('b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embeds,\n",
    "                    weights_init,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, bias_init), name=\"relu\")\n",
    "                tf.summary.histogram(\"relu\", h)\n",
    "                # h shape is [batch, n_words - fsize + 1, 1, nb_filter]\n",
    "            with tf.name_scope('%s-maxpool-fsize-%s' % (kmax, fsize)):\n",
    "                # k-maxpooling over the outputs\n",
    "                trans = tf.transpose(h, perm=[0, 2, 3, 1])\n",
    "                values, indices = tf.nn.top_k(trans, k=kmax, sorted=False)\n",
    "                pooled = tf.transpose(values, perm=[0, 3, 1, 2])\n",
    "                # pooled shape is [batch, kmax, 1, nb_filter]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        with tf.name_scope('concat_and_vectorize'):\n",
    "            # Combine all the pooled features\n",
    "            h_pool = tf.concat(pooled_outputs, 3)\n",
    "            # h_pool shape is [batch, kmax, 1, nb_filter*len(filter_sizes)]\n",
    "\n",
    "            # Vectorize filters for each sent to get sent embeddings\n",
    "            trans = tf.transpose(h_pool, perm=[0, 2, 3, 1])\n",
    "            batch = tf.shape(embeds)[0]\n",
    "            sent_embed = tf.reshape(trans, [batch, -1, 1])\n",
    "            # sent_embed shape is [batch, kmax*nb_filter*len(filter_sizes), 1]\n",
    "\n",
    "        return sent_embed\n",
    "\n",
    "    def _create_sharable_weights(self, filter_sizes, embedding_size,\n",
    "                                 nb_filter):\n",
    "        \"\"\" Create sharable weights for each type of convolution \"\"\"\n",
    "        with tf.name_scope('sharable_weights'):\n",
    "            for fsize in filter_sizes:\n",
    "                with tf.variable_scope(\"conv_weights_fsize-%s\" % fsize):\n",
    "                    filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                    weights_init = tf.get_variable(\n",
    "                        'W',\n",
    "                        initializer=tf.truncated_normal(\n",
    "                            filter_shape, stddev=0.1))\n",
    "                    bias_init = tf.get_variable(\n",
    "                        'b', initializer=tf.constant(0.1, shape=[nb_filter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T07:49:37.984368Z",
     "start_time": "2017-08-23T07:49:35.930542Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 32918.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['../data/corpus/5984d139b6b113440d63850d.txt',\n",
       "  '../data/corpus/5984c783b6b11367c5638508.txt',\n",
       "  '../data/corpus/5984cb09b6b1130b98638529.txt'],\n",
       " ['../data/corpus/5984d139b6b113440d63850d.txt',\n",
       "  '../data/corpus/5984cc18b6b11318d1638546.txt',\n",
       "  '../data/corpus/5984b6abb6b113168c63850f.txt'],\n",
       " ['../data/corpus/5984d977b6b11315fc63853a.txt',\n",
       "  '../data/corpus/5984c753b6b1136591638533.txt',\n",
       "  '../data/corpus/5984d428b6b113639c63852e.txt'],\n",
       " ['../data/corpus/5984d977b6b11315fc63853a.txt',\n",
       "  '../data/corpus/5984c233b6b1132dbd638519.txt',\n",
       "  '../data/corpus/5984b75ab6b113230c638547.txt'],\n",
       " ['../data/corpus/5984bb8fb6b1135afd638512.txt',\n",
       "  '../data/corpus/5984b8dcb6b113393f63850d.txt',\n",
       "  '../data/corpus/5984bac2b6b1134fb5638537.txt']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1000 = random.sample(list(sims), 64*2)\n",
    "triples = list(random_triples({k:sims[k] for k in ids1000}, ids, num_epochs=1))\n",
    "triples[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T07:49:37.990950Z",
     "start_time": "2017-08-23T07:49:37.986676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T07:53:45.538129Z",
     "start_time": "2017-08-23T07:53:41.120241Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "The node 'Merge/MergeSummary' has inputs from different frames. The input 'optimize/loss/sent/iter_over_docs/while/conv-5/relu_1' is in frame 'optimize/loss/sent/iter_over_docs/while/optimize/loss/sent/iter_over_docs/while/'. The input 'doc/conv_weights_fsize-3/b_0/gradient' is in frame ''.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: The node 'Merge/MergeSummary' has inputs from different frames. The input 'optimize/loss/sent/iter_over_docs/while/conv-5/relu_1' is in frame 'optimize/loss/sent/iter_over_docs/while/optimize/loss/sent/iter_over_docs/while/'. The input 'doc/conv_weights_fsize-3/b_0/gradient' is in frame ''.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-fc8e0c0ae9d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                 _, step, loss, summary = sess.run([\n\u001b[1;32m     65\u001b[0m                     \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 ])\n\u001b[1;32m     68\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: The node 'Merge/MergeSummary' has inputs from different frames. The input 'optimize/loss/sent/iter_over_docs/while/conv-5/relu_1' is in frame 'optimize/loss/sent/iter_over_docs/while/optimize/loss/sent/iter_over_docs/while/'. The input 'doc/conv_weights_fsize-3/b_0/gradient' is in frame ''."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "# X = tf.placeholder(tf.int32, [None, n_sents, n_words], name=\"X\")\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch = input_pipeline(\n",
    "            triples, batch_size=64, num_epochs=1)\n",
    "        X = tf.concat(\n",
    "            [anchor_batch, positive_batch, negative_batch],\n",
    "            axis=0,\n",
    "            name='concat_tupples')\n",
    "\n",
    "        with tf.name_scope('init_model'):\n",
    "            with tf.name_scope('batch_shape'):\n",
    "                n_sents = tf.shape(X)[1]\n",
    "                n_words = tf.shape(X)[2]\n",
    "            model = TextCNN(\n",
    "                n_sents,\n",
    "                n_words,\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                sent_filter_sizes=[2, 3, 4, 5],\n",
    "                sent_nb_filter=15,\n",
    "                doc_filter_sizes=[1, 2, 3],\n",
    "                doc_nb_filter=10,\n",
    "                sent_kmax=10,\n",
    "                doc_kmax=10)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        tf.summary.scalar(\"loss\", model.loss_op)\n",
    "        # Create summaries to visualize weights\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name.replace(':', '_'), var)\n",
    "        # Summarize all gradients\n",
    "        for grad, var in model.gradients:\n",
    "            tf.summary.histogram(var.name.replace(':', '_') + '/gradient', grad)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('../data/summary', sess.graph)\n",
    "\n",
    "        # Assign word embeddings to variable W\n",
    "        #!!!! index is shifted by 1\n",
    "        sess.run(\n",
    "            model.embedding_init,\n",
    "            feed_dict={model.embedding_placeholder: word_embeddings})\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss, summary = sess.run([\n",
    "                    train_op, model.global_step, model.loss_op,\n",
    "                    merged_summary_op\n",
    "                ])\n",
    "                current_step = tf.train.global_step(sess, model.global_step)\n",
    "                train_writer.add_summary(summary, current_step)\n",
    "                print(current_step, loss)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T09:16:07.024602Z",
     "start_time": "2017-08-22T09:16:07.019031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.2293906810037"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200000*1000/(93*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T08:04:57.247864Z",
     "start_time": "2017-08-22T08:04:57.181362Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 44]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "\n",
    "        with tf.variable_scope('var'):\n",
    "            t = tf.get_variable('w', initializer=tf.constant(0), dtype=tf.int32)\n",
    "        \n",
    "        r = t*2\n",
    "        \n",
    "        with tf.variable_scope('var', reuse=True):\n",
    "            tt = tf.get_variable('w', initializer=tf.constant(0), dtype=tf.int32)\n",
    "        \n",
    "        s = tt+44\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        res = sess.run([t, r, s])\n",
    "        print(res)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('../data/summary', sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
