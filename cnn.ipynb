{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:43:00.474991Z",
     "start_time": "2017-08-24T12:42:57.597212Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from common import *\n",
    "\n",
    "import logging\n",
    "import os, time\n",
    "import tflearn\n",
    "import tflearn.helpers.summarizer as s\n",
    "from io import StringIO\n",
    "import copy\n",
    "import pickle\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:09.806071Z",
     "start_time": "2017-08-23T10:20:03.352690Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-23 10:20:03,354 : INFO : loading Word2Vec object from ../data/vectors/w2v_model_300_w10\n",
      "2017-08-23 10:20:06,546 : INFO : loading wv recursively from ../data/vectors/w2v_model_300_w10.wv.* with mmap=None\n",
      "2017-08-23 10:20:06,548 : INFO : loading syn0 from ../data/vectors/w2v_model_300_w10.wv.syn0.npy with mmap=None\n",
      "2017-08-23 10:20:06,849 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-23 10:20:06,851 : INFO : loading syn1neg from ../data/vectors/w2v_model_300_w10.syn1neg.npy with mmap=None\n",
      "2017-08-23 10:20:07,159 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-23 10:20:07,160 : INFO : loaded ../data/vectors/w2v_model_300_w10\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(join(DATA_FOLDER, 'vectors/w2v_model_300_w10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:12.062139Z",
     "start_time": "2017-08-23T10:20:09.808583Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.wv.syn0.copy()\n",
    "index2word = copy.deepcopy(w2v_model.wv.index2word)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:16.821848Z",
     "start_time": "2017-08-23T10:20:12.064429Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2word.insert(0, 'PAD')\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"wb\") as output_file:\n",
    "    pickle.dump(index2word, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.849548Z",
     "start_time": "2017-08-23T10:20:16.824501Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    680760.000000\n",
       "mean          0.193705\n",
       "std           0.070574\n",
       "min           0.000910\n",
       "25%           0.137368\n",
       "50%           0.178099\n",
       "75%           0.244486\n",
       "max           0.744395\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = np.apply_along_axis(np.std, 1, word_embeddings)\n",
    "pd.Series(stds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:54.857777Z",
     "start_time": "2017-08-23T10:20:54.851863Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18944918087888768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.34 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "pad_vec = np.random.uniform(-0.34,0.34, word_embeddings.shape[1])\n",
    "np.std(pad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.249141Z",
     "start_time": "2017-08-23T10:20:54.859933Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.insert(word_embeddings, 0, pad_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T10:20:55.988410Z",
     "start_time": "2017-08-23T10:20:55.251655Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(join(DATA_FOLDER, 'word_embeddings_%s.npy' % word_embeddings.shape[1]), word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:43:03.531635Z",
     "start_time": "2017-08-24T12:43:00.477535Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings = np.load(join(DATA_FOLDER, 'word_embeddings_300.npy'))\n",
    "with open(join(DATA_FOLDER, \"dictionary.pickle\"), \"rb\") as input_file:\n",
    "    index2word = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:43:07.866490Z",
     "start_time": "2017-08-24T12:43:03.533975Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = glob('../data/corpus/**.txt')\n",
    "with open(join(DATA_FOLDER, 'sims.json'), 'r') as f:\n",
    "    sims = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:43:07.881437Z",
     "start_time": "2017-08-24T12:43:07.868615Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random select nagative examples\n",
    "import random\n",
    "\n",
    "def full_name(_id):\n",
    "    return join(DATA_FOLDER, 'corpus/%s.txt' % _id)\n",
    "\n",
    "def random_triples(sims, ids, num_epochs=1, seed=0):\n",
    "    \"\"\"\n",
    "    Get random triples, select negatives at random in each epoch.\n",
    "    Output: [anchor, positive, negative]\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    ixs = list(range(len(ids)))\n",
    "    for ep in range(num_epochs):\n",
    "        random.shuffle(ixs)\n",
    "        it = iter(ixs)\n",
    "        for k, v in tqdm(sims.items()):\n",
    "            exclude = [full_name(i) for i in [k] + v]\n",
    "            for vi in v:\n",
    "                ix = next(it)\n",
    "                _neg = ids[ix]\n",
    "                while _neg in exclude:\n",
    "                    ix = next(it)\n",
    "                    _neg = ids[ix]\n",
    "                yield [full_name(k), full_name(vi), _neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:43:07.910080Z",
     "start_time": "2017-08-24T12:43:07.883348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(text):\n",
    "    with tf.name_scope('parse_csv'):\n",
    "        strings = tf.string_split([text], delimiter='\\n')\n",
    "        raw_nums = tf.string_split(strings.values)\n",
    "        nums = tf.string_to_number(raw_nums.values, tf.int32)\n",
    "        dense = tf.sparse_to_dense(\n",
    "            raw_nums.indices, raw_nums.dense_shape, nums, default_value=0)\n",
    "        dense.set_shape(raw_nums.get_shape())\n",
    "    return dense\n",
    "\n",
    "def read_input_tuple(filename_queue):\n",
    "    with tf.name_scope('read_input_tuple'):\n",
    "        fnames = filename_queue.dequeue()\n",
    "        example = []\n",
    "        for fn in tf.unstack(fnames):\n",
    "            record_string = tf.read_file(fn)\n",
    "            arr = parse_csv(record_string)\n",
    "            example.append(arr)\n",
    "    return example\n",
    "\n",
    "def input_pipeline(triples, batch_size, num_epochs=1):\n",
    "    filename_queue = tf.train.input_producer(\n",
    "        triples, num_epochs=num_epochs, capacity=32, shuffle=True, seed=0)\n",
    "    example = read_input_tuple(filename_queue)\n",
    "\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    anchor, positive, negative = tf.train.batch(\n",
    "        example,\n",
    "        batch_size=batch_size,\n",
    "        capacity=capacity,\n",
    "        dynamic_pad=True,\n",
    "        #         allow_smaller_final_batch=True,\n",
    "        num_threads=cpu_count)\n",
    "    return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T13:35:45.227641Z",
     "start_time": "2017-08-24T13:35:45.032263Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 n_sents,\n",
    "                 n_words,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 sent_filter_sizes=[2, 3, 4, 5],\n",
    "                 sent_nb_filter=15,\n",
    "                 doc_filter_sizes=[1, 2, 3],\n",
    "                 doc_nb_filter=10,\n",
    "                 sent_kmax=10,\n",
    "                 doc_kmax=10,\n",
    "                 learning_rate=0.001):\n",
    "        self.n_sents = n_sents\n",
    "        self.n_words = n_words\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sent_filter_sizes = sent_filter_sizes\n",
    "        self.sent_nb_filter = sent_nb_filter\n",
    "        self.doc_filter_sizes = doc_filter_sizes\n",
    "        self.doc_nb_filter = doc_nb_filter\n",
    "        self.sent_kmax = sent_kmax\n",
    "        self.doc_kmax = doc_kmax\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                trainable=False,\n",
    "                name=\"W\")\n",
    "\n",
    "            self.embedding_placeholder = tf.placeholder(\n",
    "                tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = W.assign(self.embedding_placeholder)\n",
    "\n",
    "            embedded_words = tf.nn.embedding_lookup(W, X)\n",
    "            self.embedded_words_expanded = tf.expand_dims(embedded_words, -1)\n",
    "\n",
    "        with tf.variable_scope('sent'):\n",
    "            self._create_sharable_weights(sent_filter_sizes, embedding_size,\n",
    "                                          sent_nb_filter)\n",
    "            self.sent_embedding_size = tf.convert_to_tensor(\n",
    "                sent_kmax * sent_nb_filter * len(sent_filter_sizes))\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            self._create_sharable_weights(doc_filter_sizes,\n",
    "                                          self.sent_embedding_size.eval(),\n",
    "                                          doc_nb_filter)\n",
    "            self.doc_embedding_size = tf.convert_to_tensor(\n",
    "                doc_kmax * doc_nb_filter * len(doc_filter_sizes))\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\" This is the forward calculation from batch X to doc embeddins \"\"\"\n",
    "        with tf.variable_scope('sent'):\n",
    "\n",
    "            def convolv_on_sents(embeds):\n",
    "                return self._convolv_on_embeddings(\n",
    "                    embeds, self.sent_filter_sizes, self.sent_nb_filter,\n",
    "                    self.sent_kmax)\n",
    "\n",
    "            # iter over each document\n",
    "            self.sent_embed = tf.map_fn(\n",
    "                convolv_on_sents,\n",
    "                self.embedded_words_expanded,\n",
    "                parallel_iterations=10,\n",
    "                name='iter_over_docs')\n",
    "            # sent_embed shape is [batch, n_sent, sent_sent_kmax*sent_nb_filter*len(sent_filter_sizes), 1]\n",
    "\n",
    "        with tf.variable_scope('doc'):\n",
    "            # finally, convolv on documents\n",
    "            self.doc_embed = self._convolv_on_embeddings(\n",
    "                self.sent_embed, self.doc_filter_sizes, self.doc_nb_filter,\n",
    "                self.doc_kmax)\n",
    "            # doc_embed shape is [batch, doc_kmax*doc_nb_filter*len(doc_filter_sizes), 1]\n",
    "\n",
    "        doc_embed_normalized = tf.nn.l2_normalize(\n",
    "            self.doc_embed, dim=1, name='L2_nomalization')\n",
    "\n",
    "        return doc_embed_normalized\n",
    "\n",
    "    def loss(self, X):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            doc_embed_normalized = self.inference(X)\n",
    "            self.anchor, self.positive, self.negative = tf.unstack(\n",
    "                tf.reshape(doc_embed_normalized, [-1, 3, self.doc_embedding_size]),\n",
    "                3, 1)\n",
    "            _loss = self.triplet_loss(self.anchor, self.positive, self.negative)\n",
    "        return _loss\n",
    "\n",
    "    def optimize(self, X):\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            self.loss_op = self.loss(X)\n",
    "            self.gradients = self.optimizer.compute_gradients(self.loss_op)\n",
    "            apply_gradient_op = self.optimizer.apply_gradients(\n",
    "                self.gradients, global_step=self.global_step)\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def triplet_loss(self,\n",
    "                     anchor_embed,\n",
    "                     positive_embed,\n",
    "                     negative_embed,\n",
    "                     margin=0.2):\n",
    "        \"\"\"\n",
    "        input: Three L2 normalized tensors of shape [None, dim], compute on a batch\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('triplet_loss'):\n",
    "            d_pos = tf.reduce_sum(tf.square(anchor_embed - positive_embed), 1)\n",
    "            d_neg = tf.reduce_sum(tf.square(anchor_embed - negative_embed), 1)\n",
    "\n",
    "            loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _convolv_on_embeddings(self, embeds, filter_sizes, nb_filter, kmax):\n",
    "        \"\"\"\n",
    "        Create a convolution + k-max pool layer for each filter size, then concat and vectorize.\n",
    "        embeds shape is [batch, (n_words or n_sents), embedding_size, 1]\n",
    "        \"\"\"\n",
    "        pooled_outputs = []\n",
    "        for fsize in filter_sizes:\n",
    "            with tf.name_scope(\"conv-%s\" % fsize):\n",
    "                with tf.variable_scope(\n",
    "                        \"conv_weights_fsize-%s\" % fsize, reuse=True):\n",
    "                    weights_init = tf.get_variable('W')\n",
    "                    bias_init = tf.get_variable('b')\n",
    "                conv = tf.nn.conv2d(\n",
    "                    embeds,\n",
    "                    weights_init,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, bias_init), name=\"relu\")\n",
    "#                 h shape is [batch, n_words - fsize + 1, 1, nb_filter]\n",
    "#             tf.summary.histogram(\"relu\", h)\n",
    "\n",
    "            with tf.name_scope('%s-maxpool-fsize-%s' % (kmax, fsize)):\n",
    "                # k-maxpooling over the outputs\n",
    "                trans = tf.transpose(h, perm=[0, 2, 3, 1])\n",
    "                values, indices = tf.nn.top_k(trans, k=kmax, sorted=False)\n",
    "                pooled = tf.transpose(values, perm=[0, 3, 1, 2])\n",
    "                # pooled shape is [batch, kmax, 1, nb_filter]\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        with tf.name_scope('concat_and_vectorize'):\n",
    "            # Combine all the pooled features\n",
    "            h_pool = tf.concat(pooled_outputs, 3)\n",
    "            # h_pool shape is [batch, kmax, 1, nb_filter*len(filter_sizes)]\n",
    "\n",
    "            # Vectorize filters for each sent to get sent embeddings\n",
    "            trans = tf.transpose(h_pool, perm=[0, 2, 3, 1])\n",
    "            batch = tf.shape(embeds)[0]\n",
    "            sent_embed = tf.reshape(trans, [batch, -1, 1])\n",
    "            # sent_embed shape is [batch, kmax*nb_filter*len(filter_sizes), 1]\n",
    "\n",
    "        return sent_embed\n",
    "\n",
    "    def _create_sharable_weights(self, filter_sizes, embedding_size,\n",
    "                                 nb_filter):\n",
    "        \"\"\" Create sharable weights for each type of convolution \"\"\"\n",
    "        with tf.name_scope('sharable_weights'):\n",
    "            for fsize in filter_sizes:\n",
    "                with tf.variable_scope(\"conv_weights_fsize-%s\" % fsize):\n",
    "                    filter_shape = [fsize, embedding_size, 1, nb_filter]\n",
    "                    initializer = tf.contrib.layers.xavier_initializer_conv2d(\n",
    "                        uniform=True)\n",
    "#                     initializer=tf.truncated_normal(stddev=0.1))\n",
    "                    weights_init = tf.get_variable(\n",
    "                        'W', filter_shape, initializer=initializer)\n",
    "                    bias_init = tf.get_variable(\n",
    "                        'b', initializer=tf.constant(0.1, shape=[nb_filter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T14:14:21.924167Z",
     "start_time": "2017-08-24T14:14:21.095433Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 33246.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'../data/corpus/5984d3b6b6b113606463854a.txt',\n",
       " u'../data/corpus/5984c352b6b1133916638557.txt',\n",
       " '../data/corpus/5984bec0b6b1137ffd63853c.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1000 = random.sample(list(sims), 64*1)\n",
    "triples = list(random_triples({k:sims[k] for k in ids1000}, ids, num_epochs=1, seed=1))\n",
    "# triples = list(random_triples(sims, ids, num_epochs=1, seed=1))\n",
    "triples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T14:14:21.948223Z",
     "start_time": "2017-08-24T14:14:21.943851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T14:13:06.182797Z",
     "start_time": "2017-08-24T13:38:37.148359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_embedding_size 600\n",
      "doc_embedding_size 300\n",
      "(1, 0.19784962)\n",
      "(2, 0.14899077)\n",
      "(3, 0.11837974)\n",
      "(4, 0.075537249)\n",
      "(5, 0.070400655)\n",
      "(6, 0.048459113)\n",
      "(7, 0.072426863)\n",
      "(8, 0.058529153)\n",
      "(9, 0.07032001)\n",
      "(10, 0.043717641)\n",
      "(11, 0.067084298)\n",
      "(12, 0.042879179)\n",
      "(13, 0.056159317)\n",
      "(14, 0.062621586)\n",
      "(15, 0.037456259)\n",
      "(16, 0.055249818)\n",
      "(17, 0.042201355)\n",
      "(18, 0.04687649)\n",
      "(19, 0.045019004)\n",
      "(20, 0.075342849)\n",
      "(21, 0.027724126)\n",
      "(22, 0.065467134)\n",
      "(23, 0.029703762)\n",
      "(24, 0.029334594)\n",
      "(25, 0.022143673)\n",
      "(26, 0.039828807)\n",
      "(27, 0.030221077)\n",
      "(28, 0.05111289)\n",
      "(29, 0.03238875)\n",
      "(30, 0.028451938)\n",
      "(31, 0.048153389)\n",
      "(32, 0.015990945)\n",
      "(33, 0.027589619)\n",
      "(34, 0.043433316)\n",
      "(35, 0.02552641)\n",
      "(36, 0.039369408)\n",
      "(37, 0.03692456)\n",
      "(38, 0.031548228)\n",
      "(39, 0.024745114)\n",
      "(40, 0.024723463)\n",
      "(41, 0.029812574)\n",
      "(42, 0.024676625)\n",
      "(43, 0.037247866)\n",
      "(44, 0.03528057)\n",
      "(45, 0.028050125)\n",
      "(46, 0.019636046)\n",
      "(47, 0.048454065)\n",
      "(48, 0.023321439)\n",
      "(49, 0.03473537)\n",
      "(50, 0.035249934)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2969a2f31ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                 _, step, loss, summary = sess.run([\n\u001b[1;32m     74\u001b[0m                     \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 ])\n\u001b[1;32m     77\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vocab_size, embedding_size = word_embeddings.shape\n",
    "n_sents, n_words = 123, 40\n",
    "# X = tf.placeholder(tf.int32, [None, n_sents, n_words], name=\"X\")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.set_random_seed(0)\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True, log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        anchor_batch, positive_batch, negative_batch = input_pipeline(\n",
    "            triples, batch_size=64, num_epochs=1)\n",
    "        X = tf.reshape(\n",
    "            tf.transpose(\n",
    "                [anchor_batch, positive_batch, negative_batch], [1, 0, 2, 3],\n",
    "                name='transpose_tuples'), [-1, n_sents, n_words])\n",
    "\n",
    "        with tf.name_scope('init_model'):\n",
    "            model = TextCNN(\n",
    "                n_sents,\n",
    "                n_words,\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                sent_filter_sizes=[3, 4, 5],\n",
    "                sent_nb_filter=50,\n",
    "                doc_filter_sizes=[1, 2, 3],\n",
    "                doc_nb_filter=10,\n",
    "                sent_kmax=4,\n",
    "                doc_kmax=10,\n",
    "                learning_rate=0.001)\n",
    "        train_op = model.optimize(X)\n",
    "\n",
    "        init_local = tf.local_variables_initializer()\n",
    "        init_global = tf.global_variables_initializer()\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        print('sent_embedding_size %s' % model.sent_embedding_size.eval())\n",
    "        print('doc_embedding_size %s' % model.doc_embedding_size.eval())\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # ===================summary====================\n",
    "        tf.summary.scalar(\"loss\", model.loss_op)\n",
    "        tf.summary.histogram(\"anchor\", model.anchor)\n",
    "        tf.summary.histogram(\"positive\", model.positive)\n",
    "        tf.summary.histogram(\"negative\", model.negative)\n",
    "        # Create summaries to visualize weights\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name.replace(':', '_'), var)\n",
    "        # Summarize all gradients\n",
    "        for grad, var in model.gradients:\n",
    "            tf.summary.histogram(\n",
    "                var.name.replace(':', '_') + '/gradient', grad)\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            join(DATA_FOLDER, 'summary', 'train',\n",
    "                 str(datetime.datetime.now())), sess.graph)\n",
    "        # ===================summary====================\n",
    "\n",
    "        # Assign word embeddings to variable W\n",
    "        #!!!! index is shifted by 1\n",
    "        sess.run(\n",
    "            model.embedding_init,\n",
    "            feed_dict={model.embedding_placeholder: word_embeddings})\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                _, step, loss, summary = sess.run([\n",
    "                    train_op, model.global_step, model.loss_op,\n",
    "                    merged_summary_op\n",
    "                ])\n",
    "                current_step = tf.train.global_step(sess, model.global_step)\n",
    "                train_writer.add_summary(summary, current_step)\n",
    "                print(current_step, loss)\n",
    "\n",
    "                anchor, positive, negative = sess.run(\n",
    "                    [model.anchor, model.positive, model.negative])\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop.\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "    saver.save(\n",
    "        sess,\n",
    "        join(DATA_FOLDER, 'models', '%s' % str(datetime.datetime.now())),\n",
    "        global_step=current_step)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T09:16:07.024602Z",
     "start_time": "2017-08-22T09:16:07.019031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.2293906810037"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200000*1000/(93*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T12:33:14.889183Z",
     "start_time": "2017-08-24T12:33:14.859352Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[25 26]\n",
      "  [27 28]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]]\n",
      "\n",
      " [[29 30]\n",
      "  [31 32]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[21 22]\n",
      "  [23 24]]\n",
      "\n",
      " [[33 34]\n",
      "  [35 36]]]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(0)\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "\n",
    "        a = tf.convert_to_tensor([[[1,2],[3,4]],[[5,6],[7,8]],[[9,10],[11,12]]])\n",
    "        b = tf.convert_to_tensor([[[13,14],[15,16]],[[17,18],[19,20]],[[21,22],[23,24]]])\n",
    "        c = tf.convert_to_tensor([[[25,26],[27,28]],[[29,30],[31,32]],[[33,34],[35,36]]])\n",
    "        \n",
    "        d = tf.reshape(tf.transpose([a,b,c], [1,0,2,3]), [-1,2,2])\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        [res] = sess.run([d])\n",
    "        print(res)\n",
    "\n",
    "#         train_writer = tf.summary.FileWriter('../data/summary', sess.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
